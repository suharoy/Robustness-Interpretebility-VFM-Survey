{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2118595,"sourceType":"datasetVersion","datasetId":1271215}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VOC WSSS: Robustness Explainability Pipeline\n\nThis notebook links **explainability seeds** (CAMs/attention) to **weakly‑supervised segmentation (WSSS)** on **PASCAL VOC 2012**, and measures how **perturbations** affect both **seed robustness** and **downstream segmentation** (mIoU).\n\n**Flow**\n1) Setup & VOC paths\n2) Perturbations (flip, rotation, blur, brightness, Gaussian noise)\n3) Models: ResNet‑50 (Grad‑CAM), ViT‑B/16 (Attention Rollout)\n4) Seed generation (clean + perturbed) → saved PNG heatmaps\n5) Seed robustness metrics (IoU/SSIM)\n6) Pseudo‑masks (threshold) → minimal DeepLabV3 segmentation training\n7) mIoU evaluation & summary tables\n","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nDATA_ROOT = Path(\"/kaggle/input/pascal-voc-2012-dataset\")\nVOC_ROOT  = DATA_ROOT / \"VOC2012_train_val/VOC2012_train_val\"\n\nIMGSETS = VOC_ROOT / \"ImageSets\"\nSEG_DIR = IMGSETS / \"Segmentation\"\nACT_DIR = IMGSETS / \"Action\"\n\nOUTPUT = Path(\"./outputs\"); OUTPUT.mkdir(parents=True, exist_ok=True)\n\nprint(\"ImageSets subfolders:\", [p.name for p in IMGSETS.iterdir() if p.is_dir()])\n\n# Prefer Segmentation splits; warn if missing and (only if you really must) fall back to Action\nif (SEG_DIR / \"val.txt\").exists():\n    SPLIT_DIR = SEG_DIR\n    print(\"Using segmentation splits:\", SPLIT_DIR)\nelif (ACT_DIR / \"val.txt\").exists():\n    SPLIT_DIR = ACT_DIR\n    print(\"WARNING: 'Segmentation/val.txt' not found. Using 'Action/val.txt' (NOT suitable for mIoU).\")\nelse:\n    raise FileNotFoundError(\"No val.txt in ImageSets/Segmentation or ImageSets/Action.\")\n\ndef load_ids(split=\"val\"):\n    fp = SPLIT_DIR / f\"{split}.txt\"\n    with open(fp, \"r\") as f:\n        ids = [x.strip() for x in f]\n    return ids\n\nVAL_IDS = load_ids(\"val\")\nprint(\"val count:\", len(VAL_IDS), \"sample:\", VAL_IDS[:8])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:43:33.230240Z","iopub.execute_input":"2025-08-21T13:43:33.231053Z","iopub.status.idle":"2025-08-21T13:43:33.250570Z","shell.execute_reply.started":"2025-08-21T13:43:33.231028Z","shell.execute_reply":"2025-08-21T13:43:33.250017Z"}},"outputs":[{"name":"stdout","text":"ImageSets subfolders: ['Segmentation', 'Main', 'Layout', 'Action']\nUsing segmentation splits: /kaggle/input/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/ImageSets/Segmentation\nval count: 1449 sample: ['2007_000033', '2007_000042', '2007_000061', '2007_000123', '2007_000129', '2007_000175', '2007_000187', '2007_000323']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install timm opencv-python scikit-image tqdm matplotlib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:43:33.251579Z","iopub.execute_input":"2025-08-21T13:43:33.251854Z","iopub.status.idle":"2025-08-21T13:43:36.525590Z","shell.execute_reply.started":"2025-08-21T13:43:33.251838Z","shell.execute_reply":"2025-08-21T13:43:36.524649Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.3)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.5)\nRequirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.6.11)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (25.0)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.5.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os, json, time, math, shutil\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport torchvision as tv\nimport torchvision.transforms as T\nfrom torchvision.transforms import functional as TF\nfrom tqdm import tqdm\n\ntry:\n    import timm\nexcept:\n    print(\"Install timm: pip install timm\")\n\ntry:\n    from skimage.metrics import structural_similarity as ssim\nexcept:\n    ssim = None\n    print(\"Install scikit-image for SSIM: pip install scikit-image\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T13:43:36.526584Z","iopub.execute_input":"2025-08-21T13:43:36.526857Z","iopub.status.idle":"2025-08-21T13:43:36.533578Z","shell.execute_reply.started":"2025-08-21T13:43:36.526833Z","shell.execute_reply":"2025-08-21T13:43:36.532736Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\n\nVOC_CLASSES = [\n    \"aeroplane\",\"bicycle\",\"bird\",\"boat\",\"bottle\",\"bus\",\"car\",\"cat\",\"chair\",\"cow\",\n    \"diningtable\",\"dog\",\"horse\",\"motorbike\",\"person\",\"pottedplant\",\"sheep\",\"sofa\",\"train\",\"tvmonitor\"\n]\n\ndef load_voc_split(split=\"val\"):\n    ids_path = VOC_ROOT / \"ImageSets\" / \"Segmentation\" / f\"{split}.txt\"\n    with open(ids_path, \"r\") as f:\n        ids = [x.strip() for x in f.readlines()]\n    return ids\n\ndef voc_image_path(img_id): return VOC_ROOT / \"JPEGImages\" / f\"{img_id}.jpg\"\ndef voc_mask_path(img_id):  return VOC_ROOT / \"SegmentationClass\" / f\"{img_id}.png\"\ndef voc_xml_path(img_id):   return VOC_ROOT / \"Annotations\" / f\"{img_id}.xml\"\n\ndef voc_image_classes(img_id):\n    xml = ET.parse(voc_xml_path(img_id)).getroot()\n    return list({obj.find(\"name\").text for obj in xml.findall(\"object\")})\n\nVAL_IDS = load_voc_split(\"val\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T14:03:55.795690Z","iopub.execute_input":"2025-08-21T14:03:55.796448Z","iopub.status.idle":"2025-08-21T14:03:55.805477Z","shell.execute_reply.started":"2025-08-21T14:03:55.796423Z","shell.execute_reply":"2025-08-21T14:03:55.804957Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import cv2\n\ndef perturbations(img_pil, rot_deg=30, blur_ks=5, bright_factor=1.5, noise_std=0.1):\n    out = {}\n    out[\"hflip\"] = TF.hflip(img_pil)\n    out[\"rotation\"] = img_pil.rotate(rot_deg, resample=Image.BILINEAR, expand=False, fillcolor=(0,0,0))\n    img_cv = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n    blur = cv2.GaussianBlur(img_cv, (blur_ks, blur_ks), 0)\n    out[\"blur\"] = Image.fromarray(cv2.cvtColor(blur, cv2.COLOR_BGR2RGB))\n    out[\"brightness\"] = T.functional.adjust_brightness(img_pil, bright_factor)\n    arr = np.array(img_pil).astype(np.float32)/255.0\n    noise = np.random.normal(0, noise_std, arr.shape).astype(np.float32)\n    noisy = np.clip(arr + noise, 0, 1)\n    out[\"gauss\"] = Image.fromarray((noisy*255).astype(np.uint8))\n    return out\n\npreprocess = T.Compose([\n    T.Resize((224,224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T14:03:55.806662Z","iopub.execute_input":"2025-08-21T14:03:55.806969Z","iopub.status.idle":"2025-08-21T14:03:55.819394Z","shell.execute_reply.started":"2025-08-21T14:03:55.806953Z","shell.execute_reply":"2025-08-21T14:03:55.818696Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def load_resnet50():\n    try:\n        model = tv.models.resnet50(weights=tv.models.ResNet50_Weights.IMAGENET1K_V2)\n    except:\n        model = tv.models.resnet50(pretrained=True)\n    return model.eval().to(DEVICE)\n\ndef load_vit_b16():\n    try:\n        model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n        return model.eval().to(DEVICE)\n    except:\n        print(\"timm ViT not available\"); return None\n\nresnet = load_resnet50()\nvit    = load_vit_b16()\nprint(\"Loaded:\", bool(resnet), bool(vit))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T14:03:55.820011Z","iopub.execute_input":"2025-08-21T14:03:55.820195Z","iopub.status.idle":"2025-08-21T14:03:57.785693Z","shell.execute_reply.started":"2025-08-21T14:03:55.820180Z","shell.execute_reply":"2025-08-21T14:03:57.785043Z"}},"outputs":[{"name":"stdout","text":"Loaded: True True\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"class GradCAM:\n    def __init__(self, model, target_layer_name=\"layer4\"):\n        self.model = model; self.model.eval()\n        self.target_layer = dict([*model.named_modules()])[target_layer_name]\n        self.activations = None; self.gradients = None\n        def fwd_hook(m, inp, out): self.activations = out.detach()\n        def bwd_hook(m, gin, gout): self.gradients = gout[0].detach()\n        self.target_layer.register_forward_hook(fwd_hook)\n        self.target_layer.register_full_backward_hook(bwd_hook)\n\n    def __call__(self, img_tensor, target_class=None):\n        x = img_tensor.to(DEVICE).unsqueeze(0)\n        logits = self.model(x)\n        if target_class is None:\n            target_class = logits.argmax(dim=1).item()\n        self.model.zero_grad(set_to_none=True)\n        logits[0, target_class].backward()\n        w = self.gradients.mean(dim=(2,3), keepdim=True)\n        cam = (w * self.activations).sum(dim=1)\n        cam = torch.relu(cam); cam = cam - cam.min()\n        if cam.max() > 0: cam = cam / cam.max()\n        return cam.squeeze().cpu().numpy(), int(target_class)\n\ngradcam = GradCAM(resnet, \"layer4\")\nprint(\"Grad-CAM ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T14:03:57.787600Z","iopub.execute_input":"2025-08-21T14:03:57.787947Z","iopub.status.idle":"2025-08-21T14:03:57.795759Z","shell.execute_reply.started":"2025-08-21T14:03:57.787928Z","shell.execute_reply":"2025-08-21T14:03:57.794936Z"}},"outputs":[{"name":"stdout","text":"Grad-CAM ready.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import math\nimport torch\n\ndef vit_attention_rollout(model, x, head_fusion=\"mean\"):\n    \"\"\"\n    Robust attention rollout for timm ViTs.\n    - Hooks blk.attn.qkv\n    - Computes attention per layer (softmax(QK^T / sqrt(d)))\n    - Adds residual + row-normalize each layer\n    - Multiplies attentions across layers in order\n    - Returns a HxW heatmap for the patch grid\n    \"\"\"\n    qkv_per_layer, hooks = [], []\n\n    def hook_qkv(m, inp, out):\n        # out: [B, N, 3*dim]\n        qkv_per_layer.append(out.detach().cpu())\n\n    # Register on each transformer block\n    for blk in model.blocks:\n        h = blk.attn.qkv.register_forward_hook(hook_qkv)\n        hooks.append(h)\n\n    with torch.no_grad():\n        _ = model(x)\n\n    for h in hooks:\n        h.remove()\n\n    if len(qkv_per_layer) == 0:\n        raise RuntimeError(\"No qkv captured; check model type or hook path.\")\n\n    # Build per-layer attentions [N, N]\n    att_layers = []\n    for qkv in qkv_per_layer:\n        B, N, threeC = qkv.shape\n        assert B == 1, \"Rollout code assumes batch size 1\"\n        num_heads = model.blocks[0].attn.num_heads\n        head_dim  = (threeC // 3) // num_heads\n\n        # [B,N,3C] -> [3,B,H,N,D] -> q,k,v: [B,H,N,D]\n        qkv = qkv.reshape(B, N, 3, num_heads, head_dim).permute(2, 0, 3, 1, 4)\n        q, k = qkv[0], qkv[1]\n\n        # Attention per head: [B,H,N,N]\n        att = (q @ k.transpose(-2, -1)) / math.sqrt(head_dim)\n        att = torch.softmax(att, dim=-1)  # softmax along last dim\n\n        # Fuse heads -> [N,N]\n        if head_fusion == \"mean\":\n            att = att.mean(dim=1)[0]\n        elif head_fusion == \"max\":\n            att = att.max(dim=1)[0][0]\n        elif head_fusion == \"min\":\n            att = att.min(dim=1)[0][0]\n        else:\n            raise ValueError(\"head_fusion must be mean|max|min\")\n\n        eye = torch.eye(att.size(-1))\n        att = att + eye\n        att = att / att.sum(dim=-1, keepdim=True)\n        att_layers.append(att)\n\n    # Rollout across layers in order\n    joint = att_layers[0]\n    for l in range(1, len(att_layers)):\n        joint = att_layers[l] @ joint\n\n    # CLS-to-patch vector\n    N = joint.size(0)               \n    vec = joint[0]                  \n    def vec_to_map(v):\n        m = int(round(math.sqrt(len(v))))\n        if m * m != len(v):\n            return None\n        return v.reshape(m, m)\n\n    grid = vec[1:]                 \n    hm = vec_to_map(grid.numpy())\n    if hm is None:\n        grid2 = vec[2:]\n        hm = vec_to_map(grid2.numpy())\n        if hm is None:\n            raise ValueError(f\"Cannot reshape attention vector of length {len(vec)} \"\n                             \"to a square patch grid. Check tokens/patch size.\")\n\n    # Normalize to [0,1]\n    hm = (hm - hm.min()) / (hm.max() - hm.min() + 1e-8)\n    return hm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T14:03:57.796441Z","iopub.execute_input":"2025-08-21T14:03:57.796633Z","iopub.status.idle":"2025-08-21T14:03:57.812286Z","shell.execute_reply.started":"2025-08-21T14:03:57.796602Z","shell.execute_reply":"2025-08-21T14:03:57.811683Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import cv2, pandas as pd\n\nSEEDS_DIR = OUTPUT / \"seeds\"\n(SEEDS_DIR / \"resnet_gradcam\").mkdir(parents=True, exist_ok=True)\n(SEEDS_DIR / \"vit_attnroll\").mkdir(parents=True, exist_ok=True)\n\ndef save_heatmap(hm, out_path, size=(224,224)):\n    arr = (hm*255).astype(np.uint8)\n    if size is not None:\n        arr = cv2.resize(arr, size, interpolation=cv2.INTER_LINEAR)\n    Image.fromarray(arr).save(out_path)\n\ndef generate_seeds(img_id, do_resnet=True, do_vit=True):\n    img = Image.open(voc_image_path(img_id)).convert(\"RGB\")\n    x = preprocess(img).unsqueeze(0).to(DEVICE)\n\n    # clean\n    if do_resnet:\n        cam, _ = gradcam(x.squeeze(0))\n        save_heatmap(cam, SEEDS_DIR/\"resnet_gradcam\"/f\"{img_id}_clean.png\")\n    if do_vit and vit is not None:\n        camv = vit_attention_rollout(vit, x)\n        save_heatmap(camv, SEEDS_DIR/\"vit_attnroll\"/f\"{img_id}_clean.png\")\n\n    # perturbed\n    for name, pimg in perturbations(img).items():\n        px = preprocess(pimg).unsqueeze(0).to(DEVICE)\n        if do_resnet:\n            cam_p, _ = gradcam(px.squeeze(0))\n            save_heatmap(cam_p, SEEDS_DIR/\"resnet_gradcam\"/f\"{img_id}_{name}.png\")\n        if do_vit and vit is not None:\n            camv_p = vit_attention_rollout(vit, px)\n            save_heatmap(camv_p, SEEDS_DIR/\"vit_attnroll\"/f\"{img_id}_{name}.png\")\n\nfor img_id in tqdm(VAL_IDS, desc=\"Seeds (subset)\"):\n    generate_seeds(img_id)\nprint(\"Done subset — expand VAL_IDS for full run.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T14:03:57.813022Z","iopub.execute_input":"2025-08-21T14:03:57.813237Z","iopub.status.idle":"2025-08-21T14:14:48.838036Z","shell.execute_reply.started":"2025-08-21T14:03:57.813220Z","shell.execute_reply":"2025-08-21T14:14:48.837238Z"}},"outputs":[{"name":"stderr","text":"Seeds (subset): 100%|██████████| 1449/1449 [10:51<00:00,  2.23it/s]","output_type":"stream"},{"name":"stdout","text":"Done subset — expand VAL_IDS for full run.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"def load_gray(path): return np.array(Image.open(path).convert(\"L\")).astype(np.float32)/255.0\ndef binarize(a, thr=0.3): return (a >= thr).astype(np.uint8)\ndef iou(a, b):\n    inter = np.logical_and(a>0, b>0).sum()\n    union = np.logical_or(a>0, b>0).sum()\n    return inter/union if union>0 else 0.0\n\nPERTS = [\"hflip\",\"rotation\",\"blur\",\"brightness\",\"gauss\"]\n\ndef seed_similarity(method_dir: Path, ids):\n    rows = []\n    for img_id in ids:\n        c = load_gray(method_dir/f\"{img_id}_clean.png\")\n        cb = binarize(c, 0.3)\n        for p in PERTS:\n            pp = load_gray(method_dir/f\"{img_id}_{p}.png\")\n            pb = binarize(pp, 0.3)\n            rows.append({\n                \"id\": img_id, \"method\": method_dir.name, \"perturb\": p,\n                \"IoU\": float(iou(cb, pb)),\n                \"SSIM\": float(ssim(c, pp, data_range=1.0)) if ssim else None\n            })\n    return pd.DataFrame(rows)\n\ndf_res = seed_similarity(SEEDS_DIR/\"resnet_gradcam\", VAL_IDS)\ndf_vit = seed_similarity(SEEDS_DIR/\"vit_attnroll\", VAL_IDS) if (SEEDS_DIR/\"vit_attnroll\").exists() else pd.DataFrame()\ndf_all = pd.concat([df_res, df_vit], ignore_index=True)\ndisplay(df_all.groupby([\"method\",\"perturb\"]).agg({\"IoU\":\"mean\",\"SSIM\":\"mean\"}))\ndf_all.to_csv(OUTPUT/\"seed_similarity_subset.csv\", index=False)\nprint(\"Saved:\", (OUTPUT/\"seed_similarity_subset.csv\").resolve())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T14:14:48.838879Z","iopub.execute_input":"2025-08-21T14:14:48.839133Z","iopub.status.idle":"2025-08-21T14:15:38.368241Z","shell.execute_reply.started":"2025-08-21T14:14:48.839102Z","shell.execute_reply":"2025-08-21T14:15:38.367393Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                IoU      SSIM\nmethod         perturb                       \nresnet_gradcam blur        0.787784  0.860329\n               brightness  0.792918  0.862774\n               gauss       0.698969  0.772518\n               hflip       0.452290  0.563718\n               rotation    0.486979  0.554929\nvit_attnroll   blur        0.823758  0.974954\n               brightness  0.504413  0.823859\n               gauss       0.640053  0.904561\n               hflip       0.237995  0.579485\n               rotation    0.222032  0.497340","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>IoU</th>\n      <th>SSIM</th>\n    </tr>\n    <tr>\n      <th>method</th>\n      <th>perturb</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">resnet_gradcam</th>\n      <th>blur</th>\n      <td>0.787784</td>\n      <td>0.860329</td>\n    </tr>\n    <tr>\n      <th>brightness</th>\n      <td>0.792918</td>\n      <td>0.862774</td>\n    </tr>\n    <tr>\n      <th>gauss</th>\n      <td>0.698969</td>\n      <td>0.772518</td>\n    </tr>\n    <tr>\n      <th>hflip</th>\n      <td>0.452290</td>\n      <td>0.563718</td>\n    </tr>\n    <tr>\n      <th>rotation</th>\n      <td>0.486979</td>\n      <td>0.554929</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">vit_attnroll</th>\n      <th>blur</th>\n      <td>0.823758</td>\n      <td>0.974954</td>\n    </tr>\n    <tr>\n      <th>brightness</th>\n      <td>0.504413</td>\n      <td>0.823859</td>\n    </tr>\n    <tr>\n      <th>gauss</th>\n      <td>0.640053</td>\n      <td>0.904561</td>\n    </tr>\n    <tr>\n      <th>hflip</th>\n      <td>0.237995</td>\n      <td>0.579485</td>\n    </tr>\n    <tr>\n      <th>rotation</th>\n      <td>0.222032</td>\n      <td>0.497340</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Saved: /kaggle/working/outputs/seed_similarity_subset.csv\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"PSEUDO_DIR = OUTPUT / \"pseudo_masks\"\n(PSEUDO_DIR / \"resnet_gradcam\").mkdir(parents=True, exist_ok=True)\n(PSEUDO_DIR / \"vit_attnroll\").mkdir(parents=True, exist_ok=True)\n\ndef seed_to_mask(seed_path, thr=0.3):\n    a = load_gray(seed_path)\n    m = (a >= thr).astype(np.uint8) * 255\n    return Image.fromarray(m)\n\n# Example: clean seeds → pseudo masks (subset)\nfor img_id in VAL_IDS:\n    sp = SEEDS_DIR/\"resnet_gradcam\"/f\"{img_id}_clean.png\"\n    if sp.exists():\n        seed_to_mask(sp, thr=0.3).save(PSEUDO_DIR/\"resnet_gradcam\"/f\"{img_id}.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T14:15:38.369028Z","iopub.execute_input":"2025-08-21T14:15:38.369235Z","iopub.status.idle":"2025-08-21T14:15:40.270173Z","shell.execute_reply.started":"2025-08-21T14:15:38.369219Z","shell.execute_reply":"2025-08-21T14:15:40.269567Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from torchvision.models.segmentation import deeplabv3_resnet50\nfrom torch.utils.data import Dataset, DataLoader\n\nclass VOCPseudoDataset(Dataset):\n    def __init__(self, ids, image_root, mask_root):\n        self.ids = ids; self.image_root = image_root; self.mask_root = mask_root\n        self.img_tf = T.Compose([T.Resize((256,256)), T.ToTensor(),\n                                 T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n        self.mask_tf = T.Compose([T.Resize((256,256), interpolation=T.InterpolationMode.NEAREST)])\n\n    def __len__(self): return len(self.ids)\n    def __getitem__(self, i):\n        img_id = self.ids[i]\n        img = Image.open(self.image_root/f\"{img_id}.jpg\").convert(\"RGB\")\n        mask_p = self.mask_root/f\"{img_id}.png\"\n        if not mask_p.exists():\n            # empty mask if missing\n            mask = Image.fromarray(np.zeros((256,256), dtype=np.uint8))\n        else:\n            mask = Image.open(mask_p).convert(\"L\")\n        return self.img_tf(img), torch.from_numpy(np.array(self.mask_tf(mask))).long(), img_id\n\ntrain_ids = load_voc_split(\"trainval\")[:200]\nval_ids   = load_voc_split(\"val\")[:200]\n\ntrain_ds = VOCPseudoDataset(train_ids, VOC_ROOT/\"JPEGImages\", PSEUDO_DIR/\"resnet_gradcam\")\nval_ds   = VOCPseudoDataset(val_ids,   VOC_ROOT/\"JPEGImages\", PSEUDO_DIR/\"resnet_gradcam\")\ntrain_dl = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=2)\nval_dl   = DataLoader(val_ds,   batch_size=4, shuffle=False, num_workers=2)\n\nmodel = deeplabv3_resnet50(weights=None, num_classes=2).to(DEVICE)  \nopt = torch.optim.Adam(model.parameters(), lr=1e-4)\nce  = torch.nn.CrossEntropyLoss(ignore_index=255)\n\ndef train_one_epoch():\n    model.train(); total=0\n    for x, y, _ in train_dl:\n        x = x.to(DEVICE); y = y.to(DEVICE)\n        opt.zero_grad()\n        out = model(x)[\"out\"]\n        loss = ce(out, (y>0).long())\n        loss.backward(); opt.step()\n        total += loss.item()*x.size(0)\n    return total/len(train_dl.dataset)\n\n@torch.no_grad()\ndef evaluate_miou():\n    model.eval(); inter=0; union=0\n    for x, y, _ in val_dl:\n        x = x.to(DEVICE); y = y.to(DEVICE)\n        pred = model(x)[\"out\"].argmax(1)\n        # IoU for foreground (class 1)\n        p = (pred==1); g = (y>0)\n        inter += (p & g).sum().item()\n        union += (p | g).sum().item()\n    miou = inter/union if union>0 else 0.0\n    return miou\n\nfor ep in range(15):  \n    tl = train_one_epoch()\n    miou = evaluate_miou()\n    print(f\"Epoch {ep+1}: loss={tl:.4f}  mIoU_fg={miou:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T14:22:52.401358Z","iopub.execute_input":"2025-08-21T14:22:52.402045Z","iopub.status.idle":"2025-08-21T14:29:02.210966Z","shell.execute_reply.started":"2025-08-21T14:22:52.402015Z","shell.execute_reply":"2025-08-21T14:29:02.210081Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: loss=0.4739  mIoU_fg=0.435\nEpoch 2: loss=0.3016  mIoU_fg=0.358\nEpoch 3: loss=0.2182  mIoU_fg=0.478\nEpoch 4: loss=0.1881  mIoU_fg=0.490\nEpoch 5: loss=0.1386  mIoU_fg=0.513\nEpoch 6: loss=0.1050  mIoU_fg=0.502\nEpoch 7: loss=0.0911  mIoU_fg=0.501\nEpoch 8: loss=0.0857  mIoU_fg=0.539\nEpoch 9: loss=0.0751  mIoU_fg=0.519\nEpoch 10: loss=0.0692  mIoU_fg=0.521\nEpoch 11: loss=0.0597  mIoU_fg=0.532\nEpoch 12: loss=0.0533  mIoU_fg=0.552\nEpoch 13: loss=0.0499  mIoU_fg=0.514\nEpoch 14: loss=0.0489  mIoU_fg=0.552\nEpoch 15: loss=0.0434  mIoU_fg=0.551\n","output_type":"stream"}],"execution_count":41}]}