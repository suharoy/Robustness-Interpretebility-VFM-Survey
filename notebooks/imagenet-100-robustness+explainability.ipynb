{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T06:47:45.492672Z",
     "iopub.status.busy": "2025-08-10T06:47:45.492427Z",
     "iopub.status.idle": "2025-08-10T06:49:23.233671Z",
     "shell.execute_reply": "2025-08-10T06:49:23.233115Z",
     "shell.execute_reply.started": "2025-08-10T06:47:45.492645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install timm\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Specification and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T06:49:23.236054Z",
     "iopub.status.busy": "2025-08-10T06:49:23.235498Z",
     "iopub.status.idle": "2025-08-10T06:49:44.789404Z",
     "shell.execute_reply": "2025-08-10T06:49:44.788799Z",
     "shell.execute_reply.started": "2025-08-10T06:49:23.236030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_names = [\n",
    "    'resnet50',\n",
    "    'resnet101',\n",
    "    'densenet121',\n",
    "    'efficientnet_b3',\n",
    "    'mobilenetv3_large_100',\n",
    "    'convnext_base',\n",
    "    'regnety_032',\n",
    "    'vit_base_patch16_224',\n",
    "    'swin_base_patch4_window7_224',\n",
    "    'deit_base_patch16_224',\n",
    "    'resnext50_32x4d',\n",
    "    'inception_v3'\n",
    "]\n",
    "\n",
    "models = {}\n",
    "for name in model_names:\n",
    "    print(f'Loading {name}')\n",
    "    model = timm.create_model(name, pretrained=True).eval().to(device)\n",
    "    models[name] = model\n",
    "print(\"Loaded Models:\", list(models.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T06:49:44.790668Z",
     "iopub.status.busy": "2025-08-10T06:49:44.790306Z",
     "iopub.status.idle": "2025-08-10T06:49:44.795372Z",
     "shell.execute_reply": "2025-08-10T06:49:44.794827Z",
     "shell.execute_reply.started": "2025-08-10T06:49:44.790648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T06:49:44.796198Z",
     "iopub.status.busy": "2025-08-10T06:49:44.795991Z",
     "iopub.status.idle": "2025-08-10T06:49:46.433023Z",
     "shell.execute_reply": "2025-08-10T06:49:46.432388Z",
     "shell.execute_reply.started": "2025-08-10T06:49:44.796181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def perturb_clean(img):\n",
    "    return img\n",
    "\n",
    "def perturb_hflip(img):\n",
    "    return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "def perturb_rotation(img, angle=15):\n",
    "    return img.rotate(angle)\n",
    "\n",
    "def perturb_blur(img, radius=2):\n",
    "    return img.filter(ImageFilter.GaussianBlur(radius))\n",
    "\n",
    "def perturb_brightness(img, factor=1.5):  \n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def perturb_gaussian_noise(img, sigma=0.1):\n",
    "    arr = np.array(img).astype(np.float32) / 255.\n",
    "    noise = np.random.normal(0, sigma, arr.shape)\n",
    "    arr = np.clip(arr + noise, 0, 1)\n",
    "    return Image.fromarray((arr * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T06:49:46.434119Z",
     "iopub.status.busy": "2025-08-10T06:49:46.433846Z",
     "iopub.status.idle": "2025-08-10T06:49:46.452334Z",
     "shell.execute_reply": "2025-08-10T06:49:46.451637Z",
     "shell.execute_reply.started": "2025-08-10T06:49:46.434063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "perturbations = {\n",
    "    \"Clean\": perturb_clean,\n",
    "    \"Horizontal Flip\": perturb_hflip,\n",
    "    \"Rotation\": lambda img: perturb_rotation(img, 15),\n",
    "    \"Blur\": lambda img: perturb_blur(img, 2),\n",
    "    \"Brightness\": lambda img: perturb_brightness(img, 1.5),\n",
    "    \"Gaussian Noise\": lambda img: perturb_gaussian_noise(img, 0.1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T06:49:46.453416Z",
     "iopub.status.busy": "2025-08-10T06:49:46.453165Z",
     "iopub.status.idle": "2025-08-10T06:49:49.102879Z",
     "shell.execute_reply": "2025-08-10T06:49:49.102280Z",
     "shell.execute_reply.started": "2025-08-10T06:49:46.453394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "val_dir = '/kaggle/input/imagenet100/val.X'\n",
    "\n",
    "val_images = []\n",
    "class_names = sorted(os.listdir(val_dir))\n",
    "label_map = {cls: idx for idx, cls in enumerate(class_names)}  \n",
    "\n",
    "for cls in class_names:\n",
    "    class_folder = os.path.join(val_dir, cls)\n",
    "    img_paths = glob(os.path.join(class_folder, '*.jpg')) + glob(os.path.join(class_folder, '*.JPEG'))\n",
    "    for p in img_paths:\n",
    "        val_images.append((p, label_map[cls]))\n",
    "\n",
    "print(\"Total images loaded:\", len(val_images))\n",
    "print(\"Sample:\", val_images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T06:49:49.105113Z",
     "iopub.status.busy": "2025-08-10T06:49:49.104915Z",
     "iopub.status.idle": "2025-08-10T07:23:51.165566Z",
     "shell.execute_reply": "2025-08-10T07:23:51.164847Z",
     "shell.execute_reply.started": "2025-08-10T06:49:49.105097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    x = preprocess(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        pred = torch.argmax(logits, 1).item()\n",
    "    return pred\n",
    "\n",
    "results = {model_name: {p: [] for p in perturbations} for model_name in models}\n",
    "\n",
    "sample_size = 1000  \n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'==> Evaluating: {model_name}')\n",
    "    for pert_name, pert_fn in perturbations.items():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for img_path, label in tqdm(val_images[:sample_size], desc=f'{pert_name}', leave=False):\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = pert_fn(img)\n",
    "            pred = predict(model, img)\n",
    "            correct += (pred == label)\n",
    "            total += 1\n",
    "        acc = correct / total * 100\n",
    "        results[model_name][pert_name] = acc\n",
    "        print(f'{model_name} | {pert_name} | Acc: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:23:51.166672Z",
     "iopub.status.busy": "2025-08-10T07:23:51.166421Z",
     "iopub.status.idle": "2025-08-10T07:23:51.231520Z",
     "shell.execute_reply": "2025-08-10T07:23:51.230757Z",
     "shell.execute_reply.started": "2025-08-10T07:23:51.166647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results).T\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:23:51.232668Z",
     "iopub.status.busy": "2025-08-10T07:23:51.232394Z",
     "iopub.status.idle": "2025-08-10T07:24:02.152156Z",
     "shell.execute_reply": "2025-08-10T07:24:02.151494Z",
     "shell.execute_reply.started": "2025-08-10T07:23:51.232645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install grad-cam\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_grad_cam import GradCAM, LayerCAM, ScoreCAM, EigenCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:24:02.153577Z",
     "iopub.status.busy": "2025-08-10T07:24:02.153049Z",
     "iopub.status.idle": "2025-08-10T07:24:03.114598Z",
     "shell.execute_reply": "2025-08-10T07:24:03.113930Z",
     "shell.execute_reply.started": "2025-08-10T07:24:02.153553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = models['resnet50']\n",
    "model.eval()\n",
    "target_layer = model.layer4[-1]\n",
    "\n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01440764/ILSVRC2012_val_00000293.JPEG'\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "input_tensor = preprocess(img).unsqueeze(0).to(device)  \n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    class_idx = logits.argmax(1).item()\n",
    "\n",
    "cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(class_idx)])[0]\n",
    "\n",
    "rgb_img = np.array(img.resize((224, 224))) / 255.0\n",
    "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "plt.imshow(visualization)\n",
    "plt.title(f\"Grad-CAM: ResNet50, Class {class_idx}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:24:03.115475Z",
     "iopub.status.busy": "2025-08-10T07:24:03.115280Z",
     "iopub.status.idle": "2025-08-10T07:24:11.692616Z",
     "shell.execute_reply": "2025-08-10T07:24:11.691710Z",
     "shell.execute_reply.started": "2025-08-10T07:24:03.115460Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_resized = img.resize((224, 224))\n",
    "rgb_img = np.array(img_resized) / 255.0\n",
    "input_tensor = preprocess(img_resized).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "model = models['resnet50']\n",
    "target_layer = model.layer4[-1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    class_idx = logits.argmax(1).item()\n",
    "\n",
    "\n",
    "cam_methods = {\n",
    "    \"Original\": None,\n",
    "    \"Grad-CAM\": GradCAM,\n",
    "    \"LayerCAM\": LayerCAM,\n",
    "    \"ScoreCAM\": ScoreCAM,\n",
    "    \"EigenCAM\": EigenCAM,\n",
    "}\n",
    "\n",
    "overlays = {}\n",
    "overlays[\"Original\"] = rgb_img\n",
    "\n",
    "for cam_name, cam_class in cam_methods.items():\n",
    "    if cam_name == \"Original\":\n",
    "        continue\n",
    "    cam = cam_class(model=model, target_layers=[target_layer])\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(class_idx)])[0]\n",
    "    cam_overlay = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "    overlays[cam_name] = cam_overlay\n",
    "\n",
    "\n",
    "n_methods = len(overlays)\n",
    "plt.figure(figsize=(4 * n_methods, 4))\n",
    "for i, (title, img) in enumerate(overlays.items()):\n",
    "    plt.subplot(1, n_methods, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Overlays for resnet50\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:24:11.693564Z",
     "iopub.status.busy": "2025-08-10T07:24:11.693362Z",
     "iopub.status.idle": "2025-08-10T07:24:24.539867Z",
     "shell.execute_reply": "2025-08-10T07:24:24.539083Z",
     "shell.execute_reply.started": "2025-08-10T07:24:11.693548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_resized = img.resize((224, 224))\n",
    "rgb_img = np.array(img_resized) / 255.0\n",
    "input_tensor = preprocess(img_resized).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "model = models['resnet101']\n",
    "target_layer = model.layer4[-1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    class_idx = logits.argmax(1).item()\n",
    "\n",
    "\n",
    "cam_methods = {\n",
    "    \"Original\": None,\n",
    "    \"Grad-CAM\": GradCAM,\n",
    "    \"LayerCAM\": LayerCAM,\n",
    "    \"ScoreCAM\": ScoreCAM,\n",
    "    \"EigenCAM\": EigenCAM,\n",
    "}\n",
    "\n",
    "overlays = {}\n",
    "overlays[\"Original\"] = rgb_img\n",
    "\n",
    "for cam_name, cam_class in cam_methods.items():\n",
    "    if cam_name == \"Original\":\n",
    "        continue\n",
    "    cam = cam_class(model=model, target_layers=[target_layer])\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(class_idx)])[0]\n",
    "    cam_overlay = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "    overlays[cam_name] = cam_overlay\n",
    "\n",
    "\n",
    "n_methods = len(overlays)\n",
    "plt.figure(figsize=(4 * n_methods, 4))\n",
    "for i, (title, img) in enumerate(overlays.items()):\n",
    "    plt.subplot(1, n_methods, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Overlays for resnet101\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:24:24.541158Z",
     "iopub.status.busy": "2025-08-10T07:24:24.540885Z",
     "iopub.status.idle": "2025-08-10T07:24:28.886565Z",
     "shell.execute_reply": "2025-08-10T07:24:28.885770Z",
     "shell.execute_reply.started": "2025-08-10T07:24:24.541138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = models['densenet121']          \n",
    "target_layer = model.features[-1]             \n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'  \n",
    "\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_resized = img.resize((224, 224))\n",
    "rgb_img = np.array(img_resized) / 255.0\n",
    "input_tensor = preprocess(img_resized).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    class_idx = logits.argmax(1).item()\n",
    "\n",
    "cam_methods = {\n",
    "    \"Original\": None,\n",
    "    \"Grad-CAM\": GradCAM,\n",
    "    \"LayerCAM\": LayerCAM,\n",
    "    \"ScoreCAM\": ScoreCAM,\n",
    "    \"EigenCAM\": EigenCAM,\n",
    "}\n",
    "overlays = {\"Original\": rgb_img}\n",
    "for cam_name, cam_class in list(cam_methods.items())[1:]:  \n",
    "    try:\n",
    "        cam = cam_class(model=model, target_layers=[target_layer])\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(class_idx)])[0]\n",
    "        cam_overlay = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "        overlays[cam_name] = cam_overlay\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {cam_name}: {e}\")\n",
    "        overlays[cam_name] = np.ones_like(rgb_img)  # blank white\n",
    "\n",
    "\n",
    "n_methods = len(overlays)\n",
    "plt.figure(figsize=(4*n_methods, 4))\n",
    "for i, (name, img) in enumerate(overlays.items()):\n",
    "    plt.subplot(1, n_methods, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(name)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Overlays for densenet121\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:24:28.887638Z",
     "iopub.status.busy": "2025-08-10T07:24:28.887414Z",
     "iopub.status.idle": "2025-08-10T07:24:30.943711Z",
     "shell.execute_reply": "2025-08-10T07:24:30.942804Z",
     "shell.execute_reply.started": "2025-08-10T07:24:28.887615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = models['efficientnet_b3']          \n",
    "target_layer = model.blocks[-1]             \n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'  \n",
    "\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_resized = img.resize((224, 224))\n",
    "rgb_img = np.array(img_resized) / 255.0\n",
    "input_tensor = preprocess(img_resized).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    class_idx = logits.argmax(1).item()\n",
    "\n",
    "cam_methods = {\n",
    "    \"Original\": None,\n",
    "    \"Grad-CAM\": GradCAM,\n",
    "    \"LayerCAM\": LayerCAM,\n",
    "    \"ScoreCAM\": ScoreCAM,\n",
    "    \"EigenCAM\": EigenCAM,\n",
    "}\n",
    "overlays = {\"Original\": rgb_img}\n",
    "for cam_name, cam_class in list(cam_methods.items())[1:]:  \n",
    "    try:\n",
    "        cam = cam_class(model=model, target_layers=[target_layer])\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(class_idx)])[0]\n",
    "        cam_overlay = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "        overlays[cam_name] = cam_overlay\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {cam_name}: {e}\")\n",
    "        overlays[cam_name] = np.ones_like(rgb_img)  \n",
    "\n",
    "n_methods = len(overlays)\n",
    "plt.figure(figsize=(4*n_methods, 4))\n",
    "for i, (name, img) in enumerate(overlays.items()):\n",
    "    plt.subplot(1, n_methods, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(name)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Overlays for efficientnet_b3\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:24:30.945316Z",
     "iopub.status.busy": "2025-08-10T07:24:30.944744Z",
     "iopub.status.idle": "2025-08-10T07:24:32.643028Z",
     "shell.execute_reply": "2025-08-10T07:24:32.642353Z",
     "shell.execute_reply.started": "2025-08-10T07:24:30.945286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = models['mobilenetv3_large_100']          \n",
    "target_layer = model.blocks[-1]             \n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'  \n",
    "\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_resized = img.resize((224, 224))\n",
    "rgb_img = np.array(img_resized) / 255.0\n",
    "input_tensor = preprocess(img_resized).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    class_idx = logits.argmax(1).item()\n",
    "\n",
    "cam_methods = {\n",
    "    \"Original\": None,\n",
    "    \"Grad-CAM\": GradCAM,\n",
    "    \"LayerCAM\": LayerCAM,\n",
    "    \"ScoreCAM\": ScoreCAM,\n",
    "    \"EigenCAM\": EigenCAM,\n",
    "}\n",
    "overlays = {\"Original\": rgb_img}\n",
    "for cam_name, cam_class in list(cam_methods.items())[1:]:  \n",
    "    try:\n",
    "        cam = cam_class(model=model, target_layers=[target_layer])\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(class_idx)])[0]\n",
    "        cam_overlay = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "        overlays[cam_name] = cam_overlay\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {cam_name}: {e}\")\n",
    "        overlays[cam_name] = np.ones_like(rgb_img)  \n",
    "\n",
    "n_methods = len(overlays)\n",
    "plt.figure(figsize=(4*n_methods, 4))\n",
    "for i, (name, img) in enumerate(overlays.items()):\n",
    "    plt.subplot(1, n_methods, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(name)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Overlays for 'mobilenetv3_large_100\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:24:32.644484Z",
     "iopub.status.busy": "2025-08-10T07:24:32.644012Z",
     "iopub.status.idle": "2025-08-10T07:24:46.926733Z",
     "shell.execute_reply": "2025-08-10T07:24:46.925869Z",
     "shell.execute_reply.started": "2025-08-10T07:24:32.644462Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = models['convnext_base']          \n",
    "target_layer = model.stages[-1].blocks[-1]             \n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'  \n",
    "\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_resized = img.resize((224, 224))\n",
    "rgb_img = np.array(img_resized) / 255.0\n",
    "input_tensor = preprocess(img_resized).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    class_idx = logits.argmax(1).item()\n",
    "\n",
    "cam_methods = {\n",
    "    \"Original\": None,\n",
    "    \"Grad-CAM\": GradCAM,\n",
    "    \"LayerCAM\": LayerCAM,\n",
    "    \"ScoreCAM\": ScoreCAM,\n",
    "    \"EigenCAM\": EigenCAM,\n",
    "}\n",
    "overlays = {\"Original\": rgb_img}\n",
    "for cam_name, cam_class in list(cam_methods.items())[1:]:  \n",
    "    try:\n",
    "        cam = cam_class(model=model, target_layers=[target_layer])\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(class_idx)])[0]\n",
    "        cam_overlay = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "        overlays[cam_name] = cam_overlay\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {cam_name}: {e}\")\n",
    "        overlays[cam_name] = np.ones_like(rgb_img)  \n",
    "\n",
    "\n",
    "n_methods = len(overlays)\n",
    "plt.figure(figsize=(4*n_methods, 4))\n",
    "for i, (name, img) in enumerate(overlays.items()):\n",
    "    plt.subplot(1, n_methods, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(name)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Overlays for convnext_base\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:24:46.928253Z",
     "iopub.status.busy": "2025-08-10T07:24:46.927698Z",
     "iopub.status.idle": "2025-08-10T07:24:59.445466Z",
     "shell.execute_reply": "2025-08-10T07:24:59.444462Z",
     "shell.execute_reply.started": "2025-08-10T07:24:46.928225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True).eval().to(device)\n",
    "target_layer = model.blocks[-1].norm1          \n",
    "\n",
    "\n",
    "def vit_reshape(tensor, h=14, w=14):\n",
    "    tensor = tensor[:, 1:, :]                  \n",
    "    B, N, C = tensor.shape\n",
    "    return tensor.transpose(1, 2).reshape(B, C, h, w)\n",
    "\n",
    "\n",
    "img_path = \"/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG\" \n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "rgb_img = np.array(img.resize((224, 224))) / 255.0\n",
    "\n",
    "preproc = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "input_tensor = preproc(img).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    class_idx = model(input_tensor).argmax(1).item()\n",
    "\n",
    "\n",
    "cam_methods = {\n",
    "    \"Original\": None,\n",
    "    \"Grad-CAM\": GradCAM,\n",
    "    \"LayerCAM\": LayerCAM,\n",
    "    \"ScoreCAM\": ScoreCAM,  \n",
    "    \"EigenCAM\": EigenCAM    \n",
    "}\n",
    "\n",
    "overlays = {\"Original\": rgb_img}\n",
    "for name, cam_cls in list(cam_methods.items())[1:]:\n",
    "    try:\n",
    "        cam = cam_cls(\n",
    "            model=model,\n",
    "            target_layers=[target_layer],\n",
    "            reshape_transform=vit_reshape          \n",
    "        )\n",
    "        grayscale = cam(input_tensor=input_tensor,\n",
    "                        targets=[ClassifierOutputTarget(class_idx)])[0]\n",
    "        overlays[name] = show_cam_on_image(rgb_img, grayscale, use_rgb=True)\n",
    "    except Exception as e:\n",
    "        print(f\"{name} failed: {e}\")\n",
    "        overlays[name] = np.ones_like(rgb_img)     \n",
    "\n",
    "n = len(overlays)\n",
    "plt.figure(figsize=(4*n, 4))\n",
    "for i, (title, img_) in enumerate(overlays.items(), 1):\n",
    "    plt.subplot(1, n, i)\n",
    "    plt.imshow(img_)\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Overlays for vit_base_patch16_224\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:24:59.446507Z",
     "iopub.status.busy": "2025-08-10T07:24:59.446298Z",
     "iopub.status.idle": "2025-08-10T07:25:16.360046Z",
     "shell.execute_reply": "2025-08-10T07:25:16.359034Z",
     "shell.execute_reply.started": "2025-08-10T07:24:59.446490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True).eval().to(device)\n",
    "target_layer = model.layers[-1].blocks[-1].norm1     \n",
    "\n",
    "\n",
    "def swin_reshape(t):\n",
    "    if t.ndim == 3:                       \n",
    "        B, L, C = t.shape\n",
    "        H = W = int(L ** 0.5)             \n",
    "        return t.transpose(1,2).reshape(B, C, H, W)\n",
    "    elif t.ndim == 4:                     \n",
    "        return t.permute(0, 3, 1, 2)      \n",
    "    else:\n",
    "        raise ValueError(\"Unexpected tensor shape\", t.shape)\n",
    "\n",
    "\n",
    "img_path = \"/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "rgb = np.array(img.resize((224, 224))) / 255.0\n",
    "prep = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "tensor = prep(img).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cls_idx = model(tensor).argmax(1).item()\n",
    "print(\"pred class:\", cls_idx)\n",
    "\n",
    "\n",
    "cams = {\n",
    "    \"Original\": None,\n",
    "    \"Grad-CAM\":  GradCAM,\n",
    "    \"LayerCAM\": LayerCAM,\n",
    "    \"ScoreCAM\": ScoreCAM,    \n",
    "    \"EigenCAM\": EigenCAM     \n",
    "}\n",
    "\n",
    "overlay = {\"Original\": rgb}\n",
    "for name, cam_cls in list(cams.items())[1:]:\n",
    "    try:\n",
    "        cam = cam_cls(\n",
    "            model=model,\n",
    "            target_layers=[target_layer],\n",
    "            reshape_transform=swin_reshape\n",
    "        )\n",
    "        gray = cam(tensor, targets=[ClassifierOutputTarget(cls_idx)])[0]\n",
    "        overlay[name] = show_cam_on_image(rgb, gray, use_rgb=True)\n",
    "    except Exception as e:\n",
    "        print(f\"{name} failed: {e}\")\n",
    "        overlay[name] = np.ones_like(rgb)      \n",
    "\n",
    "\n",
    "plt.figure(figsize=(4*len(overlay), 4))\n",
    "for i, (k, v) in enumerate(overlay.items(), 1):\n",
    "    plt.subplot(1, len(overlay), i)\n",
    "    plt.imshow(v); plt.title(k); plt.axis(\"off\")\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"Overlays for swin_base_patch4_window7_224\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:25:16.360986Z",
     "iopub.status.busy": "2025-08-10T07:25:16.360794Z",
     "iopub.status.idle": "2025-08-10T07:25:28.988862Z",
     "shell.execute_reply": "2025-08-10T07:25:28.987993Z",
     "shell.execute_reply.started": "2025-08-10T07:25:16.360972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = timm.create_model(\"deit_base_patch16_224\", pretrained=True).eval().to(device)\n",
    "target_layer = model.blocks[-1].norm1         \n",
    "\n",
    "\n",
    "def vit_reshape(t, h=14, w=14):\n",
    "    t = t[:, 1:, :]                \n",
    "    B, N, C = t.shape\n",
    "    return t.transpose(1, 2).reshape(B, C, h, w)\n",
    "\n",
    "\n",
    "img_path = \"/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "rgb = np.array(img.resize((224,224))) / 255.0\n",
    "prep = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "tensor = prep(img).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    cls_idx = model(tensor).argmax(1).item()\n",
    "\n",
    "\n",
    "cams = {\n",
    "    \"Original\": None,\n",
    "    \"Grad-CAM\": GradCAM,\n",
    "    \"LayerCAM\": LayerCAM,\n",
    "    \"ScoreCAM\": ScoreCAM,\n",
    "    \"EigenCAM\": EigenCAM,\n",
    "}\n",
    "overlay = {\"Original\": rgb}\n",
    "for name, cam_cls in list(cams.items())[1:]:\n",
    "    try:\n",
    "        cam = cam_cls(model, [target_layer], reshape_transform=vit_reshape)\n",
    "        gray = cam(tensor, targets=[ClassifierOutputTarget(cls_idx)])[0]\n",
    "        overlay[name] = show_cam_on_image(rgb, gray, use_rgb=True)\n",
    "    except Exception as e:\n",
    "        print(f\"{name} failed: {e}\")\n",
    "        overlay[name] = np.ones_like(rgb)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4*len(overlay),4))\n",
    "for i,(k,v) in enumerate(overlay.items(),1):\n",
    "    plt.subplot(1,len(overlay),i); plt.imshow(v); plt.title(k); plt.axis(\"off\")\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"Overlays for deit_base_patch16_224\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:25:28.989996Z",
     "iopub.status.busy": "2025-08-10T07:25:28.989783Z",
     "iopub.status.idle": "2025-08-10T07:25:39.295838Z",
     "shell.execute_reply": "2025-08-10T07:25:39.295108Z",
     "shell.execute_reply.started": "2025-08-10T07:25:28.989980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = timm.create_model(\"resnext50_32x4d\", pretrained=True).eval().to(device)\n",
    "target_layer = model.layer4[-1]                \n",
    "\n",
    "\n",
    "img_path = \"/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "rgb = np.array(img.resize((224, 224))) / 255.0\n",
    "prep = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "tensor = prep(img).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    cls_idx = model(tensor).argmax(1).item()\n",
    "\n",
    "cams = {\n",
    "    \"Original\": None,\n",
    "    \"Grad-CAM\": GradCAM,\n",
    "    \"LayerCAM\": LayerCAM,\n",
    "    \"ScoreCAM\": ScoreCAM,\n",
    "    \"EigenCAM\": EigenCAM,\n",
    "}\n",
    "overlay = {\"Original\": rgb}\n",
    "for name, cam_cls in list(cams.items())[1:]:\n",
    "    try:\n",
    "        cam = cam_cls(model=model, target_layers=[target_layer])\n",
    "        gray = cam(tensor, targets=[ClassifierOutputTarget(cls_idx)])[0]\n",
    "        overlay[name] = show_cam_on_image(rgb, gray, use_rgb=True)\n",
    "    except Exception as e:\n",
    "        print(f\"{name} failed: {e}\")\n",
    "        overlay[name] = np.ones_like(rgb)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4*len(overlay), 4))\n",
    "for i, (k, v) in enumerate(overlay.items(), 1):\n",
    "    plt.subplot(1, len(overlay), i)\n",
    "    plt.imshow(v); plt.title(k); plt.axis(\"off\")\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"Overlays for resnext50_32x4d\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:25:39.296851Z",
     "iopub.status.busy": "2025-08-10T07:25:39.296639Z",
     "iopub.status.idle": "2025-08-10T07:25:49.938000Z",
     "shell.execute_reply": "2025-08-10T07:25:49.937158Z",
     "shell.execute_reply.started": "2025-08-10T07:25:39.296834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = timm.create_model(\"inception_v3\", pretrained=True).eval().to(device)\n",
    "target_layer = model.Mixed_7c           \n",
    "\n",
    "img_path = \"/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "rgb = np.array(img.resize((299, 299))) / 255.0\n",
    "prep = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "tensor = prep(img).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    cls_idx = model(tensor).argmax(1).item()\n",
    "\n",
    "\n",
    "cams = {\n",
    "    \"Original\": None,\n",
    "    \"Grad-CAM\": GradCAM,\n",
    "    \"LayerCAM\": LayerCAM,\n",
    "    \"ScoreCAM\": ScoreCAM,\n",
    "    \"EigenCAM\": EigenCAM,\n",
    "}\n",
    "overlay = {\"Original\": rgb}\n",
    "for name, cam_cls in list(cams.items())[1:]:\n",
    "    try:\n",
    "        cam = cam_cls(model=model, target_layers=[target_layer])\n",
    "        gray = cam(tensor, targets=[ClassifierOutputTarget(cls_idx)])[0]\n",
    "        overlay[name] = show_cam_on_image(rgb, gray, use_rgb=True)\n",
    "    except Exception as e:\n",
    "        print(f\"{name} failed: {e}\")\n",
    "        overlay[name] = np.ones_like(rgb)\n",
    "\n",
    "plt.figure(figsize=(4*len(overlay), 4))\n",
    "for i, (k, v) in enumerate(overlay.items(), 1):\n",
    "    plt.subplot(1, len(overlay), i)\n",
    "    plt.imshow(v); plt.title(k); plt.axis(\"off\")\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"Overlays for inception_v3\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradCAM Overlays under Perturbations (CNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:25:49.941737Z",
     "iopub.status.busy": "2025-08-10T07:25:49.941518Z",
     "iopub.status.idle": "2025-08-10T07:26:03.934183Z",
     "shell.execute_reply": "2025-08-10T07:26:03.933430Z",
     "shell.execute_reply.started": "2025-08-10T07:25:49.941720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def horizontal_flip(img):\n",
    "    return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "def rotate(img, degrees=30):\n",
    "    return img.rotate(degrees)\n",
    "\n",
    "def blur(img, radius=2):\n",
    "    return img.filter(ImageFilter.GaussianBlur(radius))\n",
    "\n",
    "def brighten(img, factor=1.5):\n",
    "    return ImageEnhance.Brightness(img).enhance(factor)\n",
    "\n",
    "def add_gaussian_noise(img, sigma=0.2):\n",
    "    arr = np.array(img) / 255.0\n",
    "    noise = np.random.normal(0, sigma, arr.shape)\n",
    "    noisy = np.clip(arr + noise, 0, 1)\n",
    "    return Image.fromarray((noisy * 255).astype('uint8'))\n",
    "\n",
    "perturbations = {\n",
    "    'Clean': lambda x: x,\n",
    "    'Horizontal Flip': horizontal_flip,\n",
    "    'Rotation': rotate,\n",
    "    'Blur': blur,\n",
    "    'Brightness': brighten,\n",
    "    'Gaussian Noise': add_gaussian_noise\n",
    "}\n",
    "\n",
    "\n",
    "img_path = \"/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG\"\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "models_and_layers = [\n",
    "    ('ResNet-50', 'resnet50',            lambda m: m.layer4[-1]),\n",
    "    ('ResNet-101', 'resnet101',          lambda m: m.layer4[-1]),\n",
    "    ('DenseNet-121', 'densenet121',      lambda m: m.features[-2]),\n",
    "    ('EfficientNet-B3', 'efficientnet_b3', lambda m: m.blocks[-1]),\n",
    "    ('MobileNetV3-L', 'mobilenetv3_large_100', lambda m: m.blocks[-1]),\n",
    "    ('ConvNeXt-Base', 'convnext_base',   lambda m: m.stages[-1].blocks[-1]),\n",
    "    ('ResNeXt50-32x4d', 'resnext50_32x4d', lambda m: m.layer4[-1]),\n",
    "    ('Inception V3', 'inception_v3',     lambda m: m.Mixed_7c),\n",
    "]\n",
    "\n",
    "def cam_similarity(cam1, cam2):\n",
    "    cam1f = cam1.flatten() / (np.linalg.norm(cam1.flatten()) + 1e-6)\n",
    "    cam2f = cam2.flatten() / (np.linalg.norm(cam2.flatten()) + 1e-6)\n",
    "    sim = 1 - cosine(cam1f, cam2f)\n",
    "    return sim\n",
    "\n",
    "for print_name, timm_name, layer_func in models_and_layers:\n",
    "    print(\"\\n\" + \"=\"*35)\n",
    "    print(f\"Evaluating Model: {print_name}\")\n",
    "    print(\"=\"*35)\n",
    "    # --- Load model ---\n",
    "    model = timm.create_model(timm_name, pretrained=True).eval().to(device)\n",
    "    target_layer = layer_func(model)\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "    input_tensor_clean = preprocess(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits_clean = model(input_tensor_clean)\n",
    "        class_idx_clean = logits_clean.argmax(1).item()\n",
    "\n",
    "    grayscale_cam_clean = cam(input_tensor=input_tensor_clean, targets=None)[0]\n",
    "    cams = {'Clean': grayscale_cam_clean}\n",
    "    accuracies = {}\n",
    "    similarities = {}\n",
    "\n",
    "    for name, perturb in perturbations.items():\n",
    "        perturbed_img = perturb(img)\n",
    "        input_tensor = preprocess(perturbed_img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor)\n",
    "            class_idx = logits.argmax(1).item()\n",
    "            # For a single image, 1 if same as clean class, else 0\n",
    "            top1_acc = 1 if class_idx == class_idx_clean else 0\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=None)[0]\n",
    "        cams[name] = grayscale_cam\n",
    "        accuracies[name] = top1_acc\n",
    "        similarities[name] = cam_similarity(grayscale_cam_clean, grayscale_cam) if name != 'Clean' else 1.0\n",
    "\n",
    "    print(f\"{'Perturbation':<18} {'Top-1 Match':<12} {'CAM Similarity':<14}\")\n",
    "    for name in perturbations.keys():\n",
    "        print(f\"{name:<18} {accuracies[name]:<12} {similarities[name]:<14.2f}\")\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(18, 4))\n",
    "    for i, name in enumerate(perturbations.keys()):\n",
    "         cam_map = cams[name]\n",
    "         perturbed_img = perturbations[name](img)\n",
    "         rgb_img = np.array(perturbed_img.resize((224, 224))) / 255.0\n",
    "         overlay = show_cam_on_image(rgb_img, cam_map, use_rgb=True)\n",
    "         plt.subplot(1, len(perturbations), i+1)\n",
    "         plt.imshow(overlay)\n",
    "         plt.title(name, fontsize=10)\n",
    "         plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{print_name} CAM overlays\", fontsize=14)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradCAM Overlays for Transformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:03.935003Z",
     "iopub.status.busy": "2025-08-10T07:26:03.934814Z",
     "iopub.status.idle": "2025-08-10T07:26:06.370373Z",
     "shell.execute_reply": "2025-08-10T07:26:06.369513Z",
     "shell.execute_reply.started": "2025-08-10T07:26:03.934989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, timm, numpy as np, matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "from torchvision import transforms\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "\n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'\n",
    "img      = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "def hflip(x): return x.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "def rot(x,d=30): return x.rotate(d)\n",
    "def gblur(x,r=2): return x.filter(ImageFilter.GaussianBlur(r))\n",
    "def bright(x,f=1.5): return ImageEnhance.Brightness(x).enhance(f)\n",
    "def gnoise(x,sigma=0.2):\n",
    "    a=np.array(x)/255.; a=np.clip(a+np.random.normal(0,sigma,a.shape),0,1)\n",
    "    return Image.fromarray((a*255).astype('uint8'))\n",
    "\n",
    "perturbs = {\n",
    "    \"Clean\"          : (lambda x: x),\n",
    "    \"Horizontal Flip\": hflip,\n",
    "    \"Rotation\"       : rot,\n",
    "    \"Blur\"           : gblur,\n",
    "    \"Brightness\"     : bright,\n",
    "    \"Gaussian Noise\" : gnoise,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = timm.create_model('vit_base_patch16_224', pretrained=True).eval().to(device)\n",
    "t_layer = model.blocks[-1].norm1          # final block norm\n",
    "\n",
    "def vit_reshape(t,h=14,w=14):\n",
    "    t=t[:,1:,:]                 # drop CLS\n",
    "    B,N,C=t.shape\n",
    "    return t.transpose(1,2).reshape(B,C,h,w)\n",
    "\n",
    "prep = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,)*3,(0.5,)*3)\n",
    "])\n",
    "\n",
    "clean_t = prep(img).unsqueeze(0).to(device)\n",
    "with torch.no_grad(): gt_lbl = model(clean_t).argmax(1).item()\n",
    "\n",
    "\n",
    "print(\"Evaluating Model: ViT-B/16\")\n",
    "print(f\"{'Perturbation':<16} {'Top 1 Match':>5} {'CAM Similarity':>6}\")\n",
    "\n",
    "\n",
    "cam_engine = GradCAM(model=model, target_layers=[t_layer], reshape_transform=vit_reshape)\n",
    "clean_cam  = cam_engine(clean_t, targets=[ClassifierOutputTarget(gt_lbl)])[0]\n",
    "clean_gray = clean_cam / clean_cam.max()\n",
    "\n",
    "overlays=[]\n",
    "for name,func in perturbs.items():\n",
    "    p_img = func(img).resize((224,224))\n",
    "    tensor= prep(p_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(): pred = model(tensor).argmax(1).item()\n",
    "    acc = int(pred==gt_lbl)\n",
    "\n",
    "    cam = cam_engine(tensor, targets=[ClassifierOutputTarget(pred)])[0]\n",
    "    cam_norm = cam/cam.max()\n",
    "\n",
    "    sim = ssim(clean_gray, cam_norm, data_range=1.0)*100\n",
    "\n",
    "    print(f\"{name:<16} {acc:>5} {sim:>6.1f}\")\n",
    "\n",
    "    overlays.append(show_cam_on_image(np.array(p_img)/255., cam, use_rgb=True))\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "for i,(nm,ov) in enumerate(zip(perturbs,overlays)):\n",
    "    ax=plt.subplot(1,len(overlays),i+1); ax.imshow(ov); ax.set_title(nm,fontsize=9); ax.axis('off')\n",
    "plt.suptitle(\"ViT-B/16  Grad-CAM overlays\",y=1.05,fontsize=13); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:06.371643Z",
     "iopub.status.busy": "2025-08-10T07:26:06.371377Z",
     "iopub.status.idle": "2025-08-10T07:26:08.666154Z",
     "shell.execute_reply": "2025-08-10T07:26:08.665346Z",
     "shell.execute_reply.started": "2025-08-10T07:26:06.371622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model    = timm.create_model('deit_base_patch16_224', pretrained=True).eval().to(device)\n",
    "t_layer  = model.blocks[-1].norm1                     # final transformer block norm\n",
    "\n",
    "def vit_reshape(t,h=14,w=14):                         \n",
    "    t = t[:,1:,:]                                     # drop CLS token\n",
    "    B,N,C = t.shape\n",
    "    return t.transpose(1,2).reshape(B,C,h,w)\n",
    "\n",
    "\n",
    "def hflip(x): return x.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "def rot(x,d=30): return x.rotate(d)\n",
    "def gblur(x,r=2): return x.filter(ImageFilter.GaussianBlur(r))\n",
    "def bright(x,f=1.5): return ImageEnhance.Brightness(x).enhance(f)\n",
    "def gnoise(x,sigma=0.2):\n",
    "    a=np.array(x)/255.; a=np.clip(a+np.random.normal(0,sigma,a.shape),0,1)\n",
    "    return Image.fromarray((a*255).astype('uint8'))\n",
    "\n",
    "perturbs = {\n",
    "    \"Clean\"          : (lambda x: x),\n",
    "    \"Horizontal Flip\": hflip,\n",
    "    \"Rotation\"       : rot,\n",
    "    \"Blur\"           : gblur,\n",
    "    \"Brightness\"     : bright,\n",
    "    \"Gaussian Noise\" : gnoise,\n",
    "}\n",
    "\n",
    "prep = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,)*3,(0.5,)*3)\n",
    "])\n",
    "\n",
    "\n",
    "clean_t = prep(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "with torch.no_grad(): gt_lbl = model(clean_t).argmax(1).item()\n",
    "\n",
    "\n",
    "cam_engine = GradCAM(model=model,\n",
    "                     target_layers=[t_layer],\n",
    "                     reshape_transform=vit_reshape)\n",
    "\n",
    "clean_cam  = cam_engine(clean_t, targets=[ClassifierOutputTarget(gt_lbl)])[0]\n",
    "clean_gray = clean_cam/clean_cam.max()\n",
    "\n",
    "print(\"Evaluating Model: DeiT-B/16\")\n",
    "print(\"Top-1 Accuracy and Grad-CAM SSIM (%)\")\n",
    "print(f\"{'Perturbation':<16} {'Acc':>5} {'Sim':>6}\")\n",
    "\n",
    "overlays=[]\n",
    "for nm,func in perturbs.items():\n",
    "    p_img = func(Image.open(img_path).convert(\"RGB\")).resize((224,224))\n",
    "    t_in  = prep(p_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(): pred = model(t_in).argmax(1).item()\n",
    "    acc = int(pred==gt_lbl)\n",
    "\n",
    "    cam = cam_engine(t_in, targets=[ClassifierOutputTarget(pred)])[0]\n",
    "    cam_norm = cam/cam.max()\n",
    "    sim = ssim(clean_gray, cam_norm, data_range=1.0)*100\n",
    "\n",
    "    print(f\"{nm:<16} {acc:>5} {sim:>6.1f}\")\n",
    "    overlays.append(show_cam_on_image(np.array(p_img)/255., cam, use_rgb=True))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "for i,(nm,ov) in enumerate(zip(perturbs,overlays)):\n",
    "    ax=plt.subplot(1,len(overlays),i+1); ax.imshow(ov); ax.set_title(nm,fontsize=9); ax.axis('off')\n",
    "plt.suptitle(\"DeiT-B/16   Grad-CAM overlays\",y=1.05,fontsize=13); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:08.667305Z",
     "iopub.status.busy": "2025-08-10T07:26:08.667037Z",
     "iopub.status.idle": "2025-08-10T07:26:11.420797Z",
     "shell.execute_reply": "2025-08-10T07:26:11.420028Z",
     "shell.execute_reply.started": "2025-08-10T07:26:08.667289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model    = timm.create_model('swin_base_patch4_window7_224', pretrained=True).eval().to(device)\n",
    "t_layer  = model.layers[-1].blocks[-1].norm1          # last Swin block norm\n",
    "\n",
    "def swin_reshape(t):\n",
    "    if t.ndim == 3:                       \n",
    "        B, L, C = t.shape\n",
    "        H = W = int(L ** 0.5)             \n",
    "        return t.transpose(1,2).reshape(B, C, H, W)\n",
    "    elif t.ndim == 4:                     \n",
    "        return t.permute(0, 3, 1, 2)      \n",
    "    else:\n",
    "        raise ValueError(\"Unexpected tensor shape\", t.shape)\n",
    "\n",
    "\n",
    "def hflip(x): return x.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "def rot(x,d=30): return x.rotate(d)\n",
    "def gblur(x,r=2): return x.filter(ImageFilter.GaussianBlur(r))\n",
    "def bright(x,f=1.5): return ImageEnhance.Brightness(x).enhance(f)\n",
    "def gnoise(x,sigma=0.2):\n",
    "    a=np.array(x)/255.; a=np.clip(a+np.random.normal(0,sigma,a.shape),0,1)\n",
    "    return Image.fromarray((a*255).astype('uint8'))\n",
    "\n",
    "perturbs = {\n",
    "    \"Clean\"          : (lambda x: x),\n",
    "    \"Horizontal Flip\": hflip,\n",
    "    \"Rotation\"       : rot,\n",
    "    \"Blur\"           : gblur,\n",
    "    \"Brightness\"     : bright,\n",
    "    \"Gaussian Noise\" : gnoise,\n",
    "}\n",
    "\n",
    "prep = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,)*3,(0.5,)*3)\n",
    "])\n",
    "\n",
    "clean_t = prep(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "with torch.no_grad(): gt_lbl = model(clean_t).argmax(1).item()\n",
    "\n",
    "cam_engine = GradCAM(model=model,\n",
    "                     target_layers=[t_layer],\n",
    "                     reshape_transform=swin_reshape)\n",
    "\n",
    "clean_cam  = cam_engine(clean_t, targets=[ClassifierOutputTarget(gt_lbl)])[0]\n",
    "clean_gray = clean_cam/clean_cam.max()\n",
    "\n",
    "print(\"Evaluating Model: Swin-B\")\n",
    "print(\"Top-1 Accuracy and Grad-CAM SSIM (%)\")\n",
    "print(f\"{'Perturbation':<16} {'Acc':>5} {'Sim':>6}\")\n",
    "\n",
    "overlays=[]\n",
    "for nm,func in perturbs.items():\n",
    "    p_img = func(Image.open(img_path).convert(\"RGB\")).resize((224,224))\n",
    "    t_in  = prep(p_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(): pred = model(t_in).argmax(1).item()\n",
    "    acc = int(pred==gt_lbl)\n",
    "\n",
    "    cam = cam_engine(t_in, targets=[ClassifierOutputTarget(pred)])[0]\n",
    "    cam_norm = cam/cam.max()\n",
    "    sim = ssim(clean_gray, cam_norm, data_range=1.0)*100\n",
    "\n",
    "    print(f\"{nm:<16} {acc:>5} {sim:>6.1f}\")\n",
    "    overlays.append(show_cam_on_image(np.array(p_img)/255., cam, use_rgb=True))\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "for i,(nm,ov) in enumerate(zip(perturbs,overlays)):\n",
    "    ax=plt.subplot(1,len(overlays),i+1); ax.imshow(ov); ax.set_title(nm,fontsize=9); ax.axis('off')\n",
    "plt.suptitle(\"Swin-B   Grad-CAM overlays\",y=1.05,fontsize=13); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Specific Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Attention Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:11.421783Z",
     "iopub.status.busy": "2025-08-10T07:26:11.421573Z",
     "iopub.status.idle": "2025-08-10T07:26:13.106549Z",
     "shell.execute_reply": "2025-08-10T07:26:13.105849Z",
     "shell.execute_reply.started": "2025-08-10T07:26:11.421767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True).eval().to(device)\n",
    "model.eval().to(device)\n",
    "img_path ='/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'  \n",
    "\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img = img.resize((224, 224))\n",
    "rgb_img = np.array(img) / 255.0\n",
    "\n",
    "preproc = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "input_tensor = preproc(img).unsqueeze(0).to(device)\n",
    "\n",
    "attn_weights = []\n",
    "\n",
    "def get_attn_hook(module, input, output):\n",
    "    x = input[0]  # [batch, tokens, embed_dim]\n",
    "    B, N, C = x.shape\n",
    "    qkv = module.qkv(x).reshape(B, N, 3, module.num_heads, C // module.num_heads)\n",
    "    q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "    attn = (q @ k.transpose(-2, -1)) * module.scale\n",
    "    attn = attn.softmax(dim=-1)\n",
    "    attn_weights.append(attn.detach().cpu())\n",
    "\n",
    "hooks = []\n",
    "for blk in model.blocks:\n",
    "    hooks.append(blk.attn.register_forward_hook(get_attn_hook))\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(input_tensor)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "attn_mat = torch.stack(attn_weights)      \n",
    "attn_mat = attn_mat.squeeze(1).mean(1)    \n",
    "\n",
    "num_tokens = attn_mat.shape[-1]\n",
    "result = torch.eye(num_tokens)\n",
    "for i in range(attn_mat.shape[0]):\n",
    "    attn = attn_mat[i] + torch.eye(num_tokens)\n",
    "    attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "    result = attn @ result\n",
    "\n",
    "mask = result[0, 1:]   # Exclude CLS token\n",
    "mask_length = mask.shape[0]\n",
    "side = int(np.sqrt(mask_length))\n",
    "\n",
    "print('Mask length:', mask_length)\n",
    "print('Calculated side:', side, 'side*side:', side*side)\n",
    "\n",
    "if side * side != mask_length:\n",
    "    raise ValueError(f\"Mask length {mask_length} is not a perfect square. Check patch size and model.\")\n",
    "\n",
    "mask = mask.reshape(side, side).numpy()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(rgb_img)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(rgb_img, alpha=0.5)\n",
    "plt.imshow(mask, cmap='jet', alpha=0.5)\n",
    "plt.title('Attention Rollout')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:13.107698Z",
     "iopub.status.busy": "2025-08-10T07:26:13.107414Z",
     "iopub.status.idle": "2025-08-10T07:26:13.459672Z",
     "shell.execute_reply": "2025-08-10T07:26:13.459013Z",
     "shell.execute_reply.started": "2025-08-10T07:26:13.107675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "\n",
    "mask_up = zoom(mask, (224/14, 224/14), order=1)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(rgb_img)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(rgb_img, alpha=0.7)\n",
    "plt.imshow(mask_up, cmap='jet', alpha=0.5)\n",
    "plt.title('Attention Rollout Overlay')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:13.460964Z",
     "iopub.status.busy": "2025-08-10T07:26:13.460507Z",
     "iopub.status.idle": "2025-08-10T07:26:15.594094Z",
     "shell.execute_reply": "2025-08-10T07:26:15.593329Z",
     "shell.execute_reply.started": "2025-08-10T07:26:13.460943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "from scipy.ndimage import zoom\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def compute_attention_rollout(model, img_pil, device):\n",
    "    preproc = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "    input_tensor = preproc(img_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    attn_weights = []\n",
    "    def get_attn_hook(module, input, output):\n",
    "        x = input[0]\n",
    "        B, N, C = x.shape\n",
    "        qkv = module.qkv(x).reshape(B, N, 3, module.num_heads, C // module.num_heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "        attn = (q @ k.transpose(-2, -1)) * module.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn_weights.append(attn.detach().cpu())\n",
    "\n",
    "    hooks = [blk.attn.register_forward_hook(get_attn_hook) for blk in model.blocks]\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_tensor)\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    attn_mat = torch.stack(attn_weights)\n",
    "    attn_mat = attn_mat.squeeze(1).mean(1)\n",
    "    num_tokens = attn_mat.shape[-1]\n",
    "    result = torch.eye(num_tokens)\n",
    "    for i in range(attn_mat.shape[0]):\n",
    "        attn = attn_mat[i] + torch.eye(num_tokens)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        result = attn @ result\n",
    "    mask = result[0, 1:]  # Exclude CLS\n",
    "    mask_length = mask.shape[0]\n",
    "    side = int(np.sqrt(mask_length))\n",
    "    if side * side != mask_length:\n",
    "        raise ValueError(f\"Mask length {mask_length} is not a perfect square. Check patch size and model.\")\n",
    "    mask = mask.reshape(side, side).numpy()\n",
    "    mask_up = zoom(mask, (224/side, 224/side), order=1)\n",
    "    return mask_up\n",
    "\n",
    "def horizontal_flip(img): return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "def rotate(img, degrees=30): return img.rotate(degrees)\n",
    "def blur(img, radius=2): return img.filter(ImageFilter.GaussianBlur(radius))\n",
    "def brighten(img, factor=1.5): return ImageEnhance.Brightness(img).enhance(factor)\n",
    "def add_gaussian_noise(img, sigma=0.2):\n",
    "    arr = np.array(img) / 255.0\n",
    "    noise = np.random.normal(0, sigma, arr.shape)\n",
    "    noisy = np.clip(arr + noise, 0, 1)\n",
    "    return Image.fromarray((noisy * 255).astype('uint8'))\n",
    "\n",
    "\n",
    "def similarity(a, b):\n",
    "    a = (a - np.mean(a)) / (np.std(a) + 1e-5)\n",
    "    b = (b - np.mean(b)) / (np.std(b) + 1e-5)\n",
    "    return np.mean(a * b)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True).eval().to(device)\n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "perturbations = {\n",
    "    'Clean': lambda x: x,\n",
    "    'Horizontal Flip': horizontal_flip,\n",
    "    'Rotation': lambda x: rotate(x, degrees=30),\n",
    "    'Blur': blur,\n",
    "    'Brightness': brighten,\n",
    "    'Gaussian Noise': add_gaussian_noise,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, func in perturbations.items():\n",
    "    pert_img = func(img)\n",
    "    mask_up = compute_attention_rollout(model, pert_img, device)\n",
    "    results[name] = mask_up\n",
    "\n",
    "\n",
    "clean_mask = results['Clean']\n",
    "sim_scores = {}\n",
    "for name, mask in results.items():\n",
    "    sim_scores[name] = similarity(clean_mask, mask) * 100  \n",
    "\n",
    "print(\"Attention Rollout Similarity (%)\")\n",
    "print(\"{:<18} {:>10}\".format(\"Perturbation\", \"Similarity\"))\n",
    "print(\"-\"*32)\n",
    "for name in results.keys():\n",
    "    print(\"{:<18} {:10.2f}\".format(name, sim_scores[name]))\n",
    "\n",
    "plt.figure(figsize=(15, 2.5))\n",
    "for i, (name, mask) in enumerate(results.items()):\n",
    "    plt.subplot(1, len(results), i+1)\n",
    "    plt.imshow(np.array(img), alpha=0.5)\n",
    "    plt.imshow(mask, cmap='jet', alpha=0.5)\n",
    "    plt.title(f\"{name}\\n{sim_scores[name]:.1f}%\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle('ViT-B/16 Attention Rollout Similarity Across Perturbations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:15.595214Z",
     "iopub.status.busy": "2025-08-10T07:26:15.594971Z",
     "iopub.status.idle": "2025-08-10T07:26:15.620582Z",
     "shell.execute_reply": "2025-08-10T07:26:15.620005Z",
     "shell.execute_reply.started": "2025-08-10T07:26:15.595198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def normalize_mask(m):\n",
    "    m = m - np.min(m)\n",
    "    m = m / (np.max(m) + 1e-5)\n",
    "    return m\n",
    "\n",
    "def ssim_similarity(a, b):\n",
    "    a = normalize_mask(a)\n",
    "    b = normalize_mask(b)\n",
    "    return ssim(a, b, data_range=1)\n",
    "\n",
    "\n",
    "clean_mask = normalize_mask(results['Clean'])\n",
    "sim_scores = {}\n",
    "for name, mask in results.items():\n",
    "    sim_scores[name] = ssim_similarity(clean_mask, normalize_mask(mask)) * 100\n",
    "    print(f\"{name}: {sim_scores[name]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:15.621626Z",
     "iopub.status.busy": "2025-08-10T07:26:15.621378Z",
     "iopub.status.idle": "2025-08-10T07:26:16.224629Z",
     "shell.execute_reply": "2025-08-10T07:26:16.223737Z",
     "shell.execute_reply.started": "2025-08-10T07:26:15.621604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,3))\n",
    "for i, (name, mask) in enumerate(results.items()):\n",
    "    plt.subplot(1, len(results), i+1)\n",
    "    plt.imshow(np.array(img), alpha=0.5)\n",
    "    plt.imshow(mask, cmap='jet', alpha=0.5)\n",
    "    plt.title(f\"{name}\\n{sim_scores[name]:.1f}%\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle('ViT-B/16 Attention Rollout Similarity Across Perturbations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeiT Attention Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:16.225991Z",
     "iopub.status.busy": "2025-08-10T07:26:16.225690Z",
     "iopub.status.idle": "2025-08-10T07:26:18.473750Z",
     "shell.execute_reply": "2025-08-10T07:26:18.472950Z",
     "shell.execute_reply.started": "2025-08-10T07:26:16.225973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def compute_attention_rollout(model, img_pil, device):\n",
    "    preproc = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "    input_tensor = preproc(img_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    attn_weights = []\n",
    "    def get_attn_hook(module, input, output):\n",
    "        x = input[0]\n",
    "        B, N, C = x.shape\n",
    "        qkv = module.qkv(x).reshape(B, N, 3, module.num_heads, C // module.num_heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)\n",
    "        attn = (q @ k.transpose(-2, -1)) * module.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn_weights.append(attn.detach().cpu())\n",
    "\n",
    "    hooks = [blk.attn.register_forward_hook(get_attn_hook) for blk in model.blocks]\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_tensor)\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    attn_mat = torch.stack(attn_weights)\n",
    "    attn_mat = attn_mat.squeeze(1).mean(1)\n",
    "    num_tokens = attn_mat.shape[-1]\n",
    "    result = torch.eye(num_tokens)\n",
    "    for i in range(attn_mat.shape[0]):\n",
    "        attn = attn_mat[i] + torch.eye(num_tokens)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        result = attn @ result\n",
    "    mask = result[0, 1:]  # Exclude CLS\n",
    "    mask_length = mask.shape[0]\n",
    "    side = int(np.sqrt(mask_length))\n",
    "    if side * side != mask_length:\n",
    "        raise ValueError(f\"Mask length {mask_length} is not a perfect square. Check patch size and model.\")\n",
    "    mask = mask.reshape(side, side).numpy()\n",
    "    mask_up = zoom(mask, (224/side, 224/side), order=1)\n",
    "    return mask_up\n",
    "\n",
    "def horizontal_flip(img): return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "def rotate(img, degrees=30): return img.rotate(degrees)\n",
    "def blur(img, radius=2): return img.filter(ImageFilter.GaussianBlur(radius))\n",
    "def brighten(img, factor=1.5): return ImageEnhance.Brightness(img).enhance(factor)\n",
    "def add_gaussian_noise(img, sigma=0.2):\n",
    "    arr = np.array(img) / 255.0\n",
    "    noise = np.random.normal(0, sigma, arr.shape)\n",
    "    noisy = np.clip(arr + noise, 0, 1)\n",
    "    return Image.fromarray((noisy * 255).astype('uint8'))\n",
    "\n",
    "\n",
    "def similarity(a, b):\n",
    "    a = (a - np.mean(a)) / (np.std(a) + 1e-5)\n",
    "    b = (b - np.mean(b)) / (np.std(b) + 1e-5)\n",
    "    return np.mean(a * b)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = timm.create_model('deit_base_patch16_224', pretrained=True).eval().to(device)\n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "perturbations = {\n",
    "    'Clean': lambda x: x,\n",
    "    'Horizontal Flip': horizontal_flip,\n",
    "    'Rotation': lambda x: rotate(x, degrees=30),\n",
    "    'Blur': blur,\n",
    "    'Brightness': brighten,\n",
    "    'Gaussian Noise': add_gaussian_noise,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, func in perturbations.items():\n",
    "    pert_img = func(img)\n",
    "    mask_up = compute_attention_rollout(model, pert_img, device)\n",
    "    results[name] = mask_up\n",
    "\n",
    "clean_mask = results['Clean']\n",
    "sim_scores = {}\n",
    "for name, mask in results.items():\n",
    "    sim_scores[name] = similarity(clean_mask, mask) * 100  # as percentage\n",
    "\n",
    "\n",
    "print(\"Attention Rollout Similarity (%)\")\n",
    "print(\"{:<18} {:>10}\".format(\"Perturbation\", \"Similarity\"))\n",
    "print(\"-\"*32)\n",
    "for name in results.keys():\n",
    "    print(\"{:<18} {:10.2f}\".format(name, sim_scores[name]))\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "for i, (name, mask) in enumerate(results.items()):\n",
    "    plt.subplot(1, len(results), i+1)\n",
    "    plt.imshow(np.array(img), alpha=0.5)\n",
    "    plt.imshow(mask, cmap='jet', alpha=0.5)\n",
    "    plt.title(f\"{name}\\n{sim_scores[name]:.1f}%\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle('DeiT/16 Attention Rollout Similarity Across Perturbations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:18.474789Z",
     "iopub.status.busy": "2025-08-10T07:26:18.474556Z",
     "iopub.status.idle": "2025-08-10T07:26:18.502269Z",
     "shell.execute_reply": "2025-08-10T07:26:18.501689Z",
     "shell.execute_reply.started": "2025-08-10T07:26:18.474771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def normalize_mask(m):\n",
    "    m = m - np.min(m)\n",
    "    m = m / (np.max(m) + 1e-5)\n",
    "    return m\n",
    "\n",
    "def ssim_similarity(a, b):\n",
    "    a = normalize_mask(a)\n",
    "    b = normalize_mask(b)\n",
    "    return ssim(a, b, data_range=1)\n",
    "\n",
    "\n",
    "clean_mask = normalize_mask(results['Clean'])\n",
    "sim_scores = {}\n",
    "for name, mask in results.items():\n",
    "    sim_scores[name] = ssim_similarity(clean_mask, normalize_mask(mask)) * 100\n",
    "    print(f\"{name}: {sim_scores[name]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:18.503184Z",
     "iopub.status.busy": "2025-08-10T07:26:18.502947Z",
     "iopub.status.idle": "2025-08-10T07:26:19.078890Z",
     "shell.execute_reply": "2025-08-10T07:26:19.078142Z",
     "shell.execute_reply.started": "2025-08-10T07:26:18.503158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,3))\n",
    "for i, (name, mask) in enumerate(results.items()):\n",
    "    plt.subplot(1, len(results), i+1)\n",
    "    plt.imshow(np.array(img), alpha=0.5)\n",
    "    plt.imshow(mask, cmap='jet', alpha=0.5)\n",
    "    plt.title(f\"{name}\\n{sim_scores[name]:.1f}%\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle('DeiT/16 Attention Rollout Similarity Across Perturbations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T07:26:19.079938Z",
     "iopub.status.busy": "2025-08-10T07:26:19.079692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "from scipy.ndimage import zoom\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def horizontal_flip(img): return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "def rotate(img, degrees=30): return img.rotate(degrees)\n",
    "def blur(img, radius=2): return img.filter(ImageFilter.GaussianBlur(radius))\n",
    "def brighten(img, factor=1.5): return ImageEnhance.Brightness(img).enhance(factor)\n",
    "def add_gaussian_noise(img, sigma=0.2):\n",
    "    arr = np.array(img) / 255.0\n",
    "    noise = np.random.normal(0, sigma, arr.shape)\n",
    "    noisy = np.clip(arr + noise, 0, 1)\n",
    "    return Image.fromarray((noisy * 255).astype('uint8'))\n",
    "\n",
    "perturbations = {\n",
    "    'Clean': lambda x: x,\n",
    "    'Horizontal Flip': horizontal_flip,\n",
    "    'Rotation': lambda x: rotate(x, degrees=30),\n",
    "    'Blur': blur,\n",
    "    'Brightness': brighten,\n",
    "    'Gaussian Noise': add_gaussian_noise,\n",
    "}\n",
    "\n",
    "\n",
    "def compute_swin_attention_rollout(model, img_pil, device, stage_idx=1):\n",
    "    preproc = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "    input_tensor = preproc(img_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    attn_weights = []\n",
    "    hooks = []\n",
    "    blocks = model.layers[stage_idx].blocks\n",
    "    for blk in blocks:\n",
    "        hooks.append(blk.attn.register_forward_hook(lambda m, i, o: attn_weights.append(o.detach().cpu())))\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_tensor)\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    \n",
    "    attn_list = []\n",
    "    for attn in attn_weights:\n",
    "        attn = attn.mean(1)[0]   \n",
    "        attn_list.append(attn)\n",
    "    attn_mat = torch.stack(attn_list)         \n",
    "    attn_mat = attn_mat.mean(0)               \n",
    "    num_tokens = attn_mat.shape[0]\n",
    "    result = torch.eye(num_tokens)\n",
    "    attn = attn_mat + torch.eye(num_tokens)\n",
    "    attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "    result = attn @ result\n",
    "    mask = result[0]                          # For Swin, all tokens are spatial\n",
    "    mask_length = mask.shape[0]\n",
    "    side = int(np.sqrt(mask_length))\n",
    "    if side * side != mask_length:\n",
    "        raise ValueError(f\"Mask length {mask_length} is not a perfect square. Got {mask_length}\")\n",
    "    mask = mask.reshape(side, side).numpy()\n",
    "    # Min-max normalization\n",
    "    mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-8)\n",
    "    mask_up = zoom(mask, (224/side, 224/side), order=1)\n",
    "    return mask_up\n",
    "\n",
    "\n",
    "def normalize_mask(mask):\n",
    "    m = mask - np.min(mask)\n",
    "    m = m / (np.max(m) - np.min(m) + 1e-6)\n",
    "    return m\n",
    "\n",
    "def similarity(a, b):\n",
    "    a = normalize_mask(a)\n",
    "    b = normalize_mask(b)\n",
    "    return ssim(a, b, data_range=1.0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = timm.create_model('swin_base_patch4_window7_224', pretrained=True).eval().to(device)\n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'\n",
    "img = Image.open(img_path).convert('RGB').resize((224, 224))\n",
    "\n",
    "results = {}\n",
    "for name, func in perturbations.items():\n",
    "    pert_img = func(img)\n",
    "    mask_up = compute_swin_attention_rollout(model, pert_img, device, stage_idx=1)  # use stage_idx=1 for 28x28, or 0 for 56x56\n",
    "    results[name] = mask_up\n",
    "\n",
    "\n",
    "clean_mask = results['Clean']\n",
    "sim_scores = {}\n",
    "for name, mask in results.items():\n",
    "    sim_scores[name] = similarity(clean_mask, mask) * 100\n",
    "\n",
    "print(\"Swin-B Attention Rollout Similarity (%)\")\n",
    "print(\"{:<18} {:>10}\".format(\"Perturbation\", \"Similarity\"))\n",
    "print(\"-\"*32)\n",
    "for name in results.keys():\n",
    "    print(\"{:<18} {:10.2f}\".format(name, sim_scores[name]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "for i, (name, mask) in enumerate(results.items()):\n",
    "    plt.subplot(1, len(results), i+1)\n",
    "    plt.imshow(np.array(img), alpha=0.5)\n",
    "    plt.imshow(mask, cmap='jet', alpha=0.5, vmin=0, vmax=1)\n",
    "    plt.title(f\"{name}\\n{sim_scores[name]:.1f}%\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Swin-B Attention Rollout Similarity Across Perturbations (stage 1, 28x28)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying on DINOV2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install dinov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install grad-cam --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.ndimage import zoom\n",
    "import torch\n",
    "\n",
    "\n",
    "def horizontal_flip(img): return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "def rotate(img, degrees=30): return img.rotate(degrees)\n",
    "def blur(img, radius=2): return img.filter(ImageFilter.GaussianBlur(radius))\n",
    "def brighten(img, factor=1.5): return ImageEnhance.Brightness(img).enhance(factor)\n",
    "def add_gaussian_noise(img, sigma=0.2):\n",
    "    arr = np.array(img) / 255.0\n",
    "    noise = np.random.normal(0, sigma, arr.shape)\n",
    "    noisy = np.clip(arr + noise, 0, 1)\n",
    "    return Image.fromarray((noisy * 255).astype('uint8'))\n",
    "\n",
    "perturbations = {\n",
    "    'Clean': lambda x: x,\n",
    "    'Horizontal Flip': horizontal_flip,\n",
    "    'Rotation': lambda x: rotate(x, degrees=30),\n",
    "    'Blur': blur,\n",
    "    'Brightness': brighten,\n",
    "    'Gaussian Noise': add_gaussian_noise,\n",
    "}\n",
    "\n",
    "\n",
    "def compute_vit_attention_rollout(model, img_pil, device):\n",
    "    preproc = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "    input_tensor = preproc(img_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    attn_weights = []\n",
    "    def get_attn_hook(module, input, output):\n",
    "        x = input[0]  \n",
    "        B, N, C = x.shape\n",
    "        qkv = module.qkv(x).reshape(B, N, 3, module.num_heads, C // module.num_heads)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)  \n",
    "        attn = (q @ k.transpose(-2, -1)) * module.scale  \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn_weights.append(attn.detach().cpu())\n",
    "\n",
    "    hooks = [blk.attn.register_forward_hook(get_attn_hook) for blk in model.blocks]\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_tensor)\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    attn_mat = torch.cat(attn_weights, dim=0)  \n",
    "    attn_mat = attn_mat.mean(1)                \n",
    "    num_tokens = attn_mat.shape[-1]\n",
    "    result = torch.eye(num_tokens)\n",
    "    for i in range(attn_mat.shape[0]):\n",
    "        attn = attn_mat[i] + torch.eye(num_tokens)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        result = attn @ result\n",
    "\n",
    "    mask = result[0, 1:]\n",
    "    mask_length = mask.shape[0]\n",
    "    side = int(np.sqrt(mask_length))\n",
    "    if side * side != mask_length:\n",
    "        raise ValueError(f\"Mask length {mask_length} is not a perfect square. Got {mask_length}\")\n",
    "    mask = mask.reshape(side, side).numpy()\n",
    "    mask_up = zoom(mask, (224/side, 224/side), order=1)\n",
    "    return mask_up\n",
    "\n",
    "def normalize_mask(mask):\n",
    "    m = mask - np.min(mask)\n",
    "    m = m / (np.max(m) - np.min(m) + 1e-6)\n",
    "    return m\n",
    "\n",
    "def ssim_similarity(a, b):\n",
    "    a = normalize_mask(a)\n",
    "    b = normalize_mask(b)\n",
    "    return ssim(a, b, data_range=1.0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = timm.create_model('vit_base_patch16_224_dino', pretrained=True).eval().to(device)\n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'\n",
    "img = Image.open(img_path).convert('RGB').resize((224, 224))\n",
    "\n",
    "results = {}\n",
    "for name, func in perturbations.items():\n",
    "    pert_img = func(img)\n",
    "    mask_up = compute_vit_attention_rollout(model, pert_img, device)\n",
    "    results[name] = mask_up\n",
    "\n",
    "# Similarity scores (SSIM)\n",
    "clean_mask = results['Clean']\n",
    "sim_scores = {}\n",
    "for name, mask in results.items():\n",
    "    sim_scores[name] = ssim_similarity(clean_mask, mask) * 100\n",
    "\n",
    "print(\"ViT-B/16 DINOv2  Attention Rollout Similarity (%)\")\n",
    "print(\"{:<18} {:>10}\".format(\"Perturbation\", \"Similarity\"))\n",
    "print(\"-\"*32)\n",
    "for name in results.keys():\n",
    "    print(\"{:<18} {:10.2f}\".format(name, sim_scores[name]))\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "for i, (name, mask) in enumerate(results.items()):\n",
    "    plt.subplot(1, len(results), i+1)\n",
    "    plt.imshow(np.array(img), alpha=0.5)\n",
    "    plt.imshow(mask, cmap='jet', alpha=0.5)\n",
    "    plt.title(f\"{name}\\n{sim_scores[name]:.1f}%\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle('ViT-B/16 DINOv2  Attention Rollout Similarity Across Perturbations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, timm, numpy as np, matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "from torchvision import transforms\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "\n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'\n",
    "img      = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "def hflip(x): return x.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "def rot(x,d=30): return x.rotate(d)\n",
    "def gblur(x,r=2): return x.filter(ImageFilter.GaussianBlur(r))\n",
    "def bright(x,f=1.5): return ImageEnhance.Brightness(x).enhance(f)\n",
    "def gnoise(x,sigma=0.2):\n",
    "    a=np.array(x)/255.; a=np.clip(a+np.random.normal(0,sigma,a.shape),0,1)\n",
    "    return Image.fromarray((a*255).astype('uint8'))\n",
    "\n",
    "perturbs = {\n",
    "    \"Clean\"          : (lambda x: x),\n",
    "    \"Horizontal Flip\": hflip,\n",
    "    \"Rotation\"       : rot,\n",
    "    \"Blur\"           : gblur,\n",
    "    \"Brightness\"     : bright,\n",
    "    \"Gaussian Noise\" : gnoise,\n",
    "}\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = timm.create_model('vit_base_patch16_224_dino', pretrained=True).eval().to(device)\n",
    "t_layer = model.blocks[-1].norm1          # final block norm\n",
    "\n",
    "def vit_reshape(t,h=14,w=14):\n",
    "    t=t[:,1:,:]                 # drop CLS\n",
    "    B,N,C=t.shape\n",
    "    return t.transpose(1,2).reshape(B,C,h,w)\n",
    "\n",
    "prep = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,)*3,(0.5,)*3)\n",
    "])\n",
    "\n",
    "clean_t = prep(img).unsqueeze(0).to(device)\n",
    "with torch.no_grad(): gt_lbl = model(clean_t).argmax(1).item()\n",
    "\n",
    "print(\"Evaluating Model: ViT-B/16 DINOv2\")\n",
    "print(f\"{'Perturbation':<16} {'Top 1 Match':>5} {'CAM Similarity':>6}\")\n",
    "\n",
    "cam_engine = GradCAM(model=model, target_layers=[t_layer], reshape_transform=vit_reshape)\n",
    "clean_cam  = cam_engine(clean_t, targets=[ClassifierOutputTarget(gt_lbl)])[0]\n",
    "clean_gray = clean_cam / clean_cam.max()\n",
    "\n",
    "overlays=[]\n",
    "for name,func in perturbs.items():\n",
    "    p_img = func(img).resize((224,224))\n",
    "    tensor= prep(p_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # accuracy\n",
    "    with torch.no_grad(): pred = model(tensor).argmax(1).item()\n",
    "    acc = int(pred==gt_lbl)\n",
    "\n",
    "    # CAM\n",
    "    cam = cam_engine(tensor, targets=[ClassifierOutputTarget(pred)])[0]\n",
    "    cam_norm = cam/cam.max()\n",
    "\n",
    "    # similarity w.r.t. clean CAM\n",
    "    sim = ssim(clean_gray, cam_norm, data_range=1.0)*100\n",
    "\n",
    "    print(f\"{name:<16} {acc:>5} {sim:>6.1f}\")\n",
    "    overlays.append(show_cam_on_image(np.array(p_img)/255., cam, use_rgb=True))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,3))\n",
    "for i,(nm,ov) in enumerate(zip(perturbs,overlays)):\n",
    "    ax=plt.subplot(1,len(overlays),i+1); ax.imshow(ov); ax.set_title(nm,fontsize=9); ax.axis('off')\n",
    "plt.suptitle(\"ViT-B/16 DINOv2  Grad-CAM overlays\",y=1.05,fontsize=13); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "timm.list_models('*dino*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chefer's et al.'s method (Transformer Attribution) on CNN and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision timm scikit-image einops matplotlib imageio pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "REPO_URL = \"https://github.com/hila-chefer/Transformer-Explainability.git\"\n",
    "ROOT     = \"/kaggle/working/Transformer-Explainability\"\n",
    "\n",
    "\n",
    "import subprocess, pathlib, importlib.util, sys, os, types, textwrap\n",
    "ROOT = pathlib.Path(ROOT)\n",
    "if not (ROOT / \"baselines\").is_dir():\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, str(ROOT)], check=True)\n",
    "print(\"Repo ready:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working/Transformer-Explainability')  \n",
    "\n",
    "import torch\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.ndimage import zoom\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from baselines.ViT.ViT_LRP import vit_base_patch16_224 as vit_LRP\n",
    "from baselines.ViT.ViT_explanation_generator import LRP\n",
    "\n",
    "\n",
    "def horizontal_flip(img): return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "def rotate(img, degrees=30): return img.rotate(degrees)\n",
    "def blur(img, radius=2): return img.filter(ImageFilter.GaussianBlur(radius))\n",
    "def brighten(img, factor=1.5): return ImageEnhance.Brightness(img).enhance(factor)\n",
    "def add_gaussian_noise(img, sigma=0.2):\n",
    "    arr = np.array(img) / 255.0\n",
    "    noise = np.random.normal(0, sigma, arr.shape)\n",
    "    noisy = np.clip(arr + noise, 0, 1)\n",
    "    return Image.fromarray((noisy * 255).astype('uint8'))\n",
    "\n",
    "perturbations = {\n",
    "    'Clean': lambda x: x,\n",
    "    'Horizontal Flip': horizontal_flip,\n",
    "    'Rotation': lambda x: rotate(x, degrees=30),\n",
    "    'Blur': blur,\n",
    "    'Brightness': brighten,\n",
    "    'Gaussian Noise': add_gaussian_noise,\n",
    "}\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = vit_LRP(pretrained=True).to(device).eval()\n",
    "attribution_generator = LRP(model)\n",
    "\n",
    "\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam\n",
    "\n",
    "\n",
    "def get_chefer_attr(img_pil):\n",
    "    img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
    "    attr_map = attribution_generator.generate_LRP(\n",
    "        img_tensor, method=\"transformer_attribution\", index=None\n",
    "    ).detach().cpu().numpy()\n",
    "    # Robust to shape (1, 196), (1, 14, 14), (14, 14)\n",
    "    if attr_map.ndim == 3:\n",
    "        if attr_map.shape[-1] == 196:\n",
    "            mask = attr_map.reshape(14, 14)\n",
    "        else:\n",
    "            mask = attr_map[0]\n",
    "    elif attr_map.ndim == 2 and attr_map.shape[-1] == 196:\n",
    "        mask = attr_map.reshape(14, 14)\n",
    "    elif attr_map.ndim == 2:\n",
    "        mask = attr_map\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unexpected attr_map shape: {attr_map.shape}\")\n",
    "    mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-6)\n",
    "    mask = zoom(mask, (224/14, 224/14), order=1)\n",
    "    return mask\n",
    "\n",
    "def ssim_similarity(a, b):\n",
    "    a = (a - np.min(a)) / (np.max(a) - np.min(a) + 1e-6)\n",
    "    b = (b - np.min(b)) / (np.max(b) - np.min(b) + 1e-6)\n",
    "    return ssim(a, b, data_range=1.0)\n",
    "\n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'\n",
    "img = Image.open(img_path).convert('RGB').resize((224, 224))\n",
    "\n",
    "results = {}\n",
    "for name, func in perturbations.items():\n",
    "    pert_img = func(img)\n",
    "    mask = get_chefer_attr(pert_img)\n",
    "    results[name] = {'mask': mask, 'img': pert_img}\n",
    "\n",
    "clean_mask = results['Clean']['mask']\n",
    "sim_scores = {}\n",
    "for name, res in results.items():\n",
    "    sim_scores[name] = ssim_similarity(clean_mask, res['mask']) * 100\n",
    "\n",
    "\n",
    "print(\"Chefer Attribution ViT (SSIM Similarity %)\")\n",
    "print(\"{:<18} {:>10}\".format(\"Perturbation\", \"Similarity\"))\n",
    "print(\"-\"*32)\n",
    "for name in results.keys():\n",
    "    print(\"{:<18} {:10.2f}\".format(name, sim_scores[name]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 3))\n",
    "for i, (name, res) in enumerate(results.items()):\n",
    "    img_np = np.array(res['img']) / 255.0\n",
    "    mask = res['mask']\n",
    "    overlay = show_cam_on_image(img_np, mask)\n",
    "    plt.subplot(1, len(results), i+1)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(f\"{name}\\n{sim_scores[name]:.1f}%\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Chefer Attribution ViT Similarity & Overlays')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working/Transformer-Explainability')  \n",
    "\n",
    "import torch\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.ndimage import zoom\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from baselines.ViT.ViT_LRP import deit_base_patch16_224 as deit_LRP\n",
    "from baselines.ViT.ViT_explanation_generator import LRP\n",
    "\n",
    "\n",
    "def horizontal_flip(img): return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "def rotate(img, degrees=30): return img.rotate(degrees)\n",
    "def blur(img, radius=2): return img.filter(ImageFilter.GaussianBlur(radius))\n",
    "def brighten(img, factor=1.5): return ImageEnhance.Brightness(img).enhance(factor)\n",
    "def add_gaussian_noise(img, sigma=0.2):\n",
    "    arr = np.array(img) / 255.0\n",
    "    noise = np.random.normal(0, sigma, arr.shape)\n",
    "    noisy = np.clip(arr + noise, 0, 1)\n",
    "    return Image.fromarray((noisy * 255).astype('uint8'))\n",
    "\n",
    "perturbations = {\n",
    "    'Clean': lambda x: x,\n",
    "    'Horizontal Flip': horizontal_flip,\n",
    "    'Rotation': lambda x: rotate(x, degrees=30),\n",
    "    'Blur': blur,\n",
    "    'Brightness': brighten,\n",
    "    'Gaussian Noise': add_gaussian_noise,\n",
    "}\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = deit_LRP(pretrained=True).to(device).eval()\n",
    "attribution_generator = LRP(model)\n",
    "\n",
    "\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam\n",
    "\n",
    "\n",
    "def get_chefer_attr(img_pil):\n",
    "    img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
    "    attr_map = attribution_generator.generate_LRP(\n",
    "        img_tensor, method=\"transformer_attribution\", index=None\n",
    "    ).detach().cpu().numpy()\n",
    "    # Robust to shape (1, 196), (1, 14, 14), (14, 14)\n",
    "    if attr_map.ndim == 3:\n",
    "        if attr_map.shape[-1] == 196:\n",
    "            mask = attr_map.reshape(14, 14)\n",
    "        else:\n",
    "            mask = attr_map[0]\n",
    "    elif attr_map.ndim == 2 and attr_map.shape[-1] == 196:\n",
    "        mask = attr_map.reshape(14, 14)\n",
    "    elif attr_map.ndim == 2:\n",
    "        mask = attr_map\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unexpected attr_map shape: {attr_map.shape}\")\n",
    "    mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-6)\n",
    "    mask = zoom(mask, (224/14, 224/14), order=1)\n",
    "    return mask\n",
    "\n",
    "def ssim_similarity(a, b):\n",
    "    a = (a - np.min(a)) / (np.max(a) - np.min(a) + 1e-6)\n",
    "    b = (b - np.min(b)) / (np.max(b) - np.min(b) + 1e-6)\n",
    "    return ssim(a, b, data_range=1.0)\n",
    "\n",
    "img_path = '/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG'\n",
    "img = Image.open(img_path).convert('RGB').resize((224, 224))\n",
    "\n",
    "results = {}\n",
    "for name, func in perturbations.items():\n",
    "    pert_img = func(img)\n",
    "    mask = get_chefer_attr(pert_img)\n",
    "    results[name] = {'mask': mask, 'img': pert_img}\n",
    "\n",
    "\n",
    "clean_mask = results['Clean']['mask']\n",
    "sim_scores = {}\n",
    "for name, res in results.items():\n",
    "    sim_scores[name] = ssim_similarity(clean_mask, res['mask']) * 100\n",
    "\n",
    "\n",
    "print(\"Chefer Attribution ViT (SSIM Similarity %)\")\n",
    "print(\"{:<18} {:>10}\".format(\"Perturbation\", \"Similarity\"))\n",
    "print(\"-\"*32)\n",
    "for name in results.keys():\n",
    "    print(\"{:<18} {:10.2f}\".format(name, sim_scores[name]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 3))\n",
    "for i, (name, res) in enumerate(results.items()):\n",
    "    img_np = np.array(res['img']) / 255.0\n",
    "    mask = res['mask']\n",
    "    overlay = show_cam_on_image(img_np, mask)\n",
    "    plt.subplot(1, len(results), i+1)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(f\"{name}\\n{sim_scores[name]:.1f}%\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Chefer Attribution DeiT Similarity & Overlays')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List all class names in the ViT_LRP.py file\n",
    "with open('/kaggle/working/Transformer-Explainability/baselines/ViT/ViT_LRP.py', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip().startswith('def '):\n",
    "            print(line.strip())\n",
    "        if line.strip().startswith('class '):\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScoreCAM LayerCAM EigenCAM on CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip  install grad-cam torchcam --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from pytorch_grad_cam import ScoreCAM, LayerCAM, EigenCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "\n",
    "models_and_layers = [\n",
    "    ('ResNet-50',        'resnet50',         lambda m: m.layer4[-1]),\n",
    "    ('ResNet-101',       'resnet101',        lambda m: m.layer4[-1]),\n",
    "    ('DenseNet-121',     'densenet121',      lambda m: m.features[-1]),\n",
    "    ('EfficientNet-B3',  'efficientnet_b3',  lambda m: m.blocks[-1]),\n",
    "    ('MobileNetV3-L',    'mobilenetv3_large_100', lambda m: m.blocks[-1]),\n",
    "    ('ConvNeXt-Base',    'convnext_base',    lambda m: m.stages[-1].blocks[-1]),\n",
    "    ('ResNeXt50',   'resnext50_32x4d',   lambda m: m.layer4[-1]),\n",
    "    ('Inception V3',     'inception_v3',     lambda m: m.Mixed_7c),\n",
    "  \n",
    "]\n",
    "\n",
    "\n",
    "def horizontal_flip(img): return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "def rotate(img, degrees=30): return img.rotate(degrees)\n",
    "def blur(img, radius=2): return img.filter(ImageFilter.GaussianBlur(radius))\n",
    "def brighten(img, factor=1.5): return ImageEnhance.Brightness(img).enhance(factor)\n",
    "def add_gaussian_noise(img, sigma=0.2):\n",
    "    arr = np.array(img) / 255.0\n",
    "    noise = np.random.normal(0, sigma, arr.shape)\n",
    "    noisy = np.clip(arr + noise, 0, 1)\n",
    "    return Image.fromarray((noisy * 255).astype('uint8'))\n",
    "\n",
    "perturbations = {\n",
    "    'Clean': lambda x: x,\n",
    "    'Horizontal Flip': horizontal_flip,\n",
    "    'Rotation': lambda x: rotate(x, degrees=30),\n",
    "    'Blur': blur,\n",
    "    'Brightness': brighten,\n",
    "    'Gaussian Noise': add_gaussian_noise,\n",
    "}\n",
    "\n",
    "\n",
    "input_size = 224\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "img_path = \"/kaggle/input/imagenet100/val.X/n01685808/ILSVRC2012_val_00023693.JPEG\"\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "\n",
    "cam_methods = {\n",
    "    'ScoreCAM': ScoreCAM,\n",
    "    'LayerCAM': LayerCAM,\n",
    "    'EigenCAM': EigenCAM\n",
    "}\n",
    "\n",
    "for cam_name, cam_class in cam_methods.items():\n",
    "    print(f\"\\n===== {cam_name} =====\")\n",
    "    for print_name, timm_name, layer_func in models_and_layers:\n",
    "        print(f\"\\n---- {print_name} ----\")\n",
    "        model = timm.create_model(timm_name, pretrained=True).eval().to(device)\n",
    "        target_layer = layer_func(model)\n",
    "        cam = cam_class(model, target_layers=[target_layer])\n",
    "\n",
    "        results, overlays = {}, {}\n",
    "\n",
    "        for pert_name, pert_func in perturbations.items():\n",
    "            pert_img = pert_func(img)\n",
    "            img_tensor = preprocess(pert_img).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(img_tensor)\n",
    "                class_idx = output.argmax().item()\n",
    "            \n",
    "            mask = cam(input_tensor=img_tensor, targets=[ClassifierOutputTarget(class_idx)])[0]\n",
    "            mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-6)\n",
    "            results[pert_name] = mask\n",
    "\n",
    "            \n",
    "            img_np = np.array(pert_img.resize((input_size, input_size))) / 255.0\n",
    "            overlay = show_cam_on_image(img_np, mask, use_rgb=True)\n",
    "            overlays[pert_name] = overlay\n",
    "\n",
    "        \n",
    "        clean_mask = results['Clean']\n",
    "        sim_scores = {n: ssim(clean_mask, m, data_range=1.0) * 100 for n, m in results.items()}\n",
    "        print(\"{:<18} {:>10}\".format(\"Perturbation\", \"Similarity\"))\n",
    "        print(\"-\"*32)\n",
    "        for n in results:\n",
    "            print(\"{:<18} {:10.2f}\".format(n, sim_scores[n]))\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(18, 3))\n",
    "        for i, n in enumerate(results):\n",
    "            plt.subplot(1, len(results), i+1)\n",
    "            plt.imshow(overlays[n])\n",
    "            plt.title(f\"{n}\\n{sim_scores[n]:.1f}%\")\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(f\"{print_name} - {cam_name} Similarity & Overlays\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1500837,
     "sourceId": 2491748,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
