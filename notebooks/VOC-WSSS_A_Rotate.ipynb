{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2118595,"sourceType":"datasetVersion","datasetId":1271215}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install albumentations==1.4.7 imgaug==0.4.0 opencv-python-headless\n\nimport os, math, gc, sys, json, random, warnings\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageOps\nimport cv2\n\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom albumentations import (\n    Compose, HorizontalFlip, RandomBrightnessContrast, GaussianBlur,\n    ShiftScaleRotate, Resize, Normalize\n)\nfrom albumentations.pytorch import ToTensorV2\n\nwarnings.filterwarnings(\"ignore\")\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\ntorch.backends.cudnn.benchmark = True\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T02:44:13.147524Z","iopub.execute_input":"2025-08-26T02:44:13.147814Z","iopub.status.idle":"2025-08-26T02:44:28.840791Z","shell.execute_reply.started":"2025-08-26T02:44:13.147788Z","shell.execute_reply":"2025-08-26T02:44:28.839969Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mDevice: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ---- dataset roots (Kaggle input) ----\nVOC_ROOT   = Path(\"/kaggle/input/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val\")\nIMG_ROOT   = VOC_ROOT / \"JPEGImages\"\nGT_ROOT    = VOC_ROOT / \"SegmentationClass\"   # 21-class masks (255=ignore)\nSPLIT_ROOT = VOC_ROOT / \"ImageSets\" / \"Segmentation\"\n\n# ---- experiment I/O ----\nEXP_NAME = \"A_rotate\"                  # <— only change for other experiments\nBASE_OUT = Path(\"/kaggle/working/outputs\") / EXP_NAME\nSEEDS_DIR   = BASE_OUT / \"seeds\"           # Grad-CAM seed overlays\nPSEUDO_DIR  = BASE_OUT / \"pseudo_masks\"    # binary masks from seeds\nCKPT_DIR    = BASE_OUT / \"ckpt\"\nRESULTS_DIR = BASE_OUT / \"results\"\nfor d in [SEEDS_DIR, PSEUDO_DIR, CKPT_DIR, RESULTS_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# quick check that VOC is visible\nprint(\"VOC exists:\", VOC_ROOT.exists(), \"| JPEGImages:\", IMG_ROOT.exists())\nprint(\"Saving to:\", BASE_OUT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T02:44:28.842207Z","iopub.execute_input":"2025-08-26T02:44:28.842543Z","iopub.status.idle":"2025-08-26T02:44:28.859124Z","shell.execute_reply.started":"2025-08-26T02:44:28.842522Z","shell.execute_reply":"2025-08-26T02:44:28.858533Z"}},"outputs":[{"name":"stdout","text":"VOC exists: True | JPEGImages: True\nSaving to: /kaggle/working/outputs/A_rotate\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def read_ids(txt: Path) -> List[str]:\n    return [x.strip() for x in open(txt) if x.strip()]\n\ntrain_ids = read_ids(SPLIT_ROOT / \"train.txt\")\nval_ids   = read_ids(SPLIT_ROOT / \"val.txt\")\n\nprint(\"Counts  → train:\", len(train_ids), \" val:\", len(val_ids))\nprint(\"Sample  →\", val_ids[0])\nprint(\"Sample image path →\", (IMG_ROOT / f\"{val_ids[0]}.jpg\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T02:44:28.859925Z","iopub.execute_input":"2025-08-26T02:44:28.860160Z","iopub.status.idle":"2025-08-26T02:44:28.898179Z","shell.execute_reply.started":"2025-08-26T02:44:28.860135Z","shell.execute_reply":"2025-08-26T02:44:28.897635Z"}},"outputs":[{"name":"stdout","text":"Counts  → train: 1464  val: 1449\nSample  → 2007_000033\nSample image path → /kaggle/input/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/JPEGImages/2007_000033.jpg\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os, sys, time, csv, math, warnings\nfrom pathlib import Path\nimport numpy as np\nimport torch, torchvision\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchvision.models import resnet50, ResNet50_Weights\n\n\ntry:\n    DEVICE\nexcept NameError:\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nPSEUDO_DIR = Path(PSEUDO_DIR) if 'PSEUDO_DIR' in globals() else Path('./outputs/pseudo_masks')\nPSEUDO_DIR.mkdir(parents=True, exist_ok=True)\n\nSEEDS_DIR = Path(SEEDS_DIR) if 'SEEDS_DIR' in globals() else Path('./outputs/seeds')  # not required but kept\n\nassert 'IMG_ROOT' in globals(), \"IMG_ROOT must be defined (folder with VOC JPEGImages).\"\nassert 'train_ids' in globals(), \"train_ids must be defined (list of VOC image ids).\"\n\nclass CAMHelper:\n    def __init__(self, device=DEVICE):\n        m = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(device).eval()\n        self.model = m\n        self.device = device\n        self._feat = []\n        self._grad = []\n\n        def f_hook(_, __, out): self._feat = [out]\n        def b_hook(_, grad_in, grad_out): self._grad = [grad_out[0]]\n\n        # hook last conv in layer4\n        self.h1 = m.layer4[-1].conv3.register_forward_hook(f_hook)\n        self.h2 = m.layer4[-1].conv3.register_full_backward_hook(b_hook)\n\n        self.pre = torchvision.transforms.Compose([\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize(mean=[0.485,0.456,0.406],\n                                             std=[0.229,0.224,0.225])\n        ])\n\n        # AMP for speed\n        self.use_amp = True\n        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n\n    @torch.inference_mode(False)\n    def run(self, pil_img: Image.Image) -> np.ndarray:\n        self._feat.clear(); self._grad.clear()\n        x = self.pre(pil_img).unsqueeze(0).to(self.device)\n        x.requires_grad_(True)\n\n        with torch.cuda.amp.autocast(enabled=self.use_amp):\n            logits = self.model(x)              # [1,1000]\n            cls = logits.argmax(1)\n            score = logits[0, cls]\n\n        # backward for CAM weights\n        self.model.zero_grad(set_to_none=True)\n        (score).backward()\n\n        A = self._feat[0][0]                   # [C,H,W]\n        G = self._grad[0][0]                   # [C,H,W]\n        w = G.mean(dim=(1,2))                  # [C]\n        cam = (w[:, None, None] * A).sum(0)\n        cam = torch.relu(cam)\n        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-6)\n        return cam.detach().float().cpu().numpy()   # [Hc, Wc]\n\n    def close(self):\n        self.h1.remove(); self.h2.remove()\n\ncam_helper = CAMHelper()\n\n# --------- main writer ----------\ndef seeds_to_pseudo(ids, rot_deg=10.0, th=0.30,\n                    overwrite=False,\n                    log_csv=PSEUDO_DIR.parent / \"seed_build_log.csv\"):\n    \"\"\"\n    Build binary pseudo masks from Grad-CAM with an optional rotation.\n    - ids: list of VOC ids (strings)\n    - rot_deg: rotate input before CAM, then center-crop back\n    - th: threshold on upsampled CAM to produce FG (>=th)\n    - overwrite: if False, skip if output PNG already exists\n    - log_csv: write per-image stats; will append if exists\n    \"\"\"\n    ids = list(ids)\n    if len(ids) == 0:\n        print(\"No ids to process.\")\n        return\n\n    # CSV logger (append-safe)\n    log_csv = Path(log_csv)\n    header = [\"img_id\", \"saved_path\", \"time_sec\", \"cam_mean\", \"cam_max\", \"threshold\", \"skipped\", \"error\"]\n    new_file = not log_csv.exists()\n    f_log = open(log_csv, \"a\", newline=\"\")\n    wlog = csv.writer(f_log)\n    if new_file: wlog.writerow(header)\n\n    t0 = time.time()\n    t_last = t0\n    proc, skipped, errors = 0, 0, 0\n\n    pbar = tqdm(ids, desc=\"Grad-CAM→pseudo\", unit=\"img\")\n    for img_id in pbar:\n        out_png = PSEUDO_DIR / f\"{img_id}.png\"\n        if out_png.exists() and not overwrite:\n            skipped += 1\n            wlog.writerow([img_id, str(out_png), 0.0, \"\", \"\", th, True, \"\"])\n            continue\n\n        # resolve input image (jpg/jpeg)\n        img_p = IMG_ROOT / f\"{img_id}.jpg\"\n        if not img_p.exists():\n            alt = IMG_ROOT / f\"{img_id}.jpeg\"\n            if alt.exists(): img_p = alt\n        if not img_p.exists():\n            errors += 1\n            wlog.writerow([img_id, \"\", 0.0, \"\", \"\", th, False, \"missing input\"])\n            continue\n\n        try:\n            # load + rotate + center-crop back to original size\n            img = Image.open(img_p).convert(\"RGB\")\n            W, H = img.size\n            if abs(rot_deg) > 1e-3:\n                rot = img.rotate(rot_deg, resample=Image.BILINEAR, expand=True, fillcolor=(0,0,0))\n                left = max((rot.width - W)//2, 0); top = max((rot.height - H)//2, 0)\n                rot = rot.crop((left, top, left+W, top+H))\n            else:\n                rot = img\n\n            # CAM\n            t1 = time.time()\n            cmap = cam_helper.run(rot)                   # [h,w]\n            cam_up = cv2.resize(cmap, (W, H), interpolation=cv2.INTER_LINEAR)\n\n            # stats + mask\n            cmean = float(cam_up.mean()); cmax = float(cam_up.max())\n            mask = (cam_up >= th).astype(np.uint8) * 255\n\n            # write (fast: no compression)\n            Image.fromarray(mask).save(out_png, compress_level=0)\n\n            # bookkeeping\n            dt = time.time() - t1\n            proc += 1\n            # rolling ETA\n            elapsed = time.time() - t0\n            per_img = elapsed / max(proc, 1)\n            remain = per_img * (len(ids) - proc - skipped)\n            pbar.set_postfix(proc=proc, skip=skipped, err=errors,\n                             sec_img=f\"{per_img:.2f}\", eta=f\"{remain/60:.1f}m\")\n\n            # log\n            wlog.writerow([img_id, str(out_png), f\"{dt:.3f}\", f\"{cmean:.5f}\", f\"{cmax:.5f}\", th, False, \"\"])\n\n            # flush every 100\n            if (proc + skipped) % 100 == 0:\n                f_log.flush()\n                sys.stdout.flush()\n\n        except Exception as e:\n            errors += 1\n            wlog.writerow([img_id, \"\", 0.0, \"\", \"\", th, False, repr(e)])\n            continue\n\n    f_log.close()\n    cam_helper.close()\n    print(f\"\\nDone. Written: {proc} | Skipped: {skipped} | Errors: {errors}\")\n    print(f\"Log: {log_csv}\")\n    print(f\"Output dir: {PSEUDO_DIR.resolve()}\")\n    # quick sample check\n    print(\"Sample outputs:\", list(map(str, list(PSEUDO_DIR.glob('*.png'))[:5])))\n\nseeds_to_pseudo(train_ids, rot_deg=10.0, th=0.30, overwrite=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T02:44:28.899361Z","iopub.execute_input":"2025-08-26T02:44:28.899536Z","iopub.status.idle":"2025-08-26T02:46:29.697625Z","shell.execute_reply.started":"2025-08-26T02:44:28.899521Z","shell.execute_reply":"2025-08-26T02:46:29.697003Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 166MB/s] \nGrad-CAM→pseudo: 100%|██████████| 1464/1464 [01:55<00:00, 12.64img/s, err=0, eta=0.0m, proc=1464, sec_img=0.08, skip=0]","output_type":"stream"},{"name":"stdout","text":"\nDone. Written: 1464 | Skipped: 0 | Errors: 0\nLog: /kaggle/working/outputs/A_rotate/seed_build_log.csv\nOutput dir: /kaggle/working/outputs/A_rotate/pseudo_masks\nSample outputs: ['/kaggle/working/outputs/A_rotate/pseudo_masks/2011_002590.png', '/kaggle/working/outputs/A_rotate/pseudo_masks/2008_002258.png', '/kaggle/working/outputs/A_rotate/pseudo_masks/2007_003000.png', '/kaggle/working/outputs/A_rotate/pseudo_masks/2009_004661.png', '/kaggle/working/outputs/A_rotate/pseudo_masks/2007_009889.png']\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"IGNORE_IDX = 255\nIMG_SIZE = 256\nBATCH_TRAIN = 8\nBATCH_VAL = 8\n\nclass VOCPseudoBinary(Dataset):\n    \"\"\"\n    Returns x (FloatTensor CxHxW), g (LongTensor HxW in {0,1}), \n            q (LongTensor HxW in {0,255}), id (str)\n    g: pseudo labels (binary), q: GT for eval (255=ignore)\n    \"\"\"\n    def __init__(self, ids, img_root, pseudo_root, gt_root, train=True, size=256):\n        self.ids = ids\n        self.img_root = Path(img_root)\n        self.pseudo_root = Path(pseudo_root)\n        self.gt_root = Path(gt_root)\n        self.train = bool(train)\n        self.size = int(size)\n\n        aug_train = Compose([\n            HorizontalFlip(p=0.5),\n            RandomBrightnessContrast(p=0.2),\n            GaussianBlur(blur_limit=(3,5), p=0.15),\n            ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n            Resize(self.size, self.size),\n            Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n            ToTensorV2()\n        ])\n        aug_val = Compose([\n            Resize(self.size, self.size),\n            Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n            ToTensorV2()\n        ])\n        self.tr = aug_train if self.train else aug_val\n\n    def __len__(self): return len(self.ids)\n\n    def __getitem__(self, i):\n        img_id = self.ids[i]\n        # image\n        ip = self.img_root / f\"{img_id}.jpg\"\n        if not ip.exists(): ip = self.img_root / f\"{img_id}.jpeg\"\n        img = np.array(Image.open(ip).convert(\"RGB\"))\n\n        # pseudo (binary)\n        pp = self.pseudo_root / f\"{img_id}.png\"\n        if pp.exists():\n            pm = np.array(Image.open(pp))\n        else:\n            pm = np.zeros(img.shape[:2], np.uint8)\n\n        # GT (21-class → we only use 'is not background' as fg; 255 ignore)\n        gp = self.gt_root / f\"{img_id}.png\"\n        if gp.exists():\n            gt = np.array(Image.open(gp))\n        else:\n            gt = np.full(img.shape[:2], IGNORE_IDX, np.uint8)\n\n        # resize masks to image size before joint transforms\n        H, W = img.shape[:2]\n        if pm.shape != (H,W): pm = cv2.resize(pm, (W,H), interpolation=cv2.INTER_NEAREST)\n        if gt.shape != (H,W): gt = cv2.resize(gt, (W,H), interpolation=cv2.INTER_NEAREST)\n\n        # binary gt for eval (fg = any class ≠ 0 & ≠ 255)\n        gbin = np.where(gt==IGNORE_IDX, IGNORE_IDX, (gt!=0).astype(np.uint8)*1)\n\n        # joint transform\n        out = self.tr(image=img, mask=pm, masks=[gbin])\n        x = out[\"image\"].float()\n        pm2 = out[\"mask\"]                       # (H,W) in {0..255}\n        gt2 = out[\"masks\"][0]                   # (H,W) in {0,1,255}\n\n        # tensor types\n        g = (torch.as_tensor(pm2).long() > 0).to(torch.long)        # 0/1\n        q = torch.as_tensor(gt2).to(torch.long)                     # 0/1/255\n\n        return x, g, q, img_id\n\ndef make_loaders():\n    train_ds = VOCPseudoBinary(train_ids, IMG_ROOT, PSEUDO_DIR, GT_ROOT, train=True, size=IMG_SIZE)\n    val_ds   = VOCPseudoBinary(val_ids, IMG_ROOT, PSEUDO_DIR, GT_ROOT, train=False, size=IMG_SIZE)\n    train_dl = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True, num_workers=2, pin_memory=True)\n    val_dl   = DataLoader(val_ds, batch_size=BATCH_VAL,   shuffle=False, num_workers=2, pin_memory=True)\n    return train_dl, val_dl\n\ntrain_dl, val_dl = make_loaders()\n\n# smoke test\nbx, bg, bq, bids = next(iter(train_dl))\nprint(\"Shapes:\", bx.shape, bg.shape, bq.shape, \"| batches →\", len(train_dl), len(val_dl))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T02:47:22.489488Z","iopub.execute_input":"2025-08-26T02:47:22.489780Z","iopub.status.idle":"2025-08-26T02:47:23.037648Z","shell.execute_reply.started":"2025-08-26T02:47:22.489755Z","shell.execute_reply":"2025-08-26T02:47:23.036804Z"}},"outputs":[{"name":"stdout","text":"Shapes: torch.Size([8, 3, 256, 256]) torch.Size([8, 256, 256]) torch.Size([8, 256, 256]) | batches → 183 182\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n\ndef build_deeplab_binary(num_classes=2):\n    try:\n        m = deeplabv3_resnet50(weights_backbone=DeepLabV3_ResNet50_Weights.DEFAULT.backbone)\n    except Exception:\n        m = deeplabv3_resnet50(weights=None)\n    m.classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1)\n    return m.to(DEVICE)\n\nmodel = build_deeplab_binary(num_classes=2)\n\n# optimizer / loss\nopt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\nscaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n\ndef ce_loss(logits, target):\n    # target in {0,1}; ignore 255\n    return F.cross_entropy(logits, target, ignore_index=IGNORE_IDX)\n\ndef fast_confusion(pred, gt, num_classes=2, ignore_index=255):\n    # pred, gt: (H,W) ints\n    mask = gt != ignore_index\n    n = num_classes\n    k = (gt[mask] * n + pred[mask]).to(torch.int64)\n    binc = torch.bincount(k, minlength=n*n).float()\n    return binc.reshape(n, n)\n\ndef iou_from_cm(cm):\n    inter = torch.diag(cm)\n    union = cm.sum(1) + cm.sum(0) - inter\n    iou = (inter / (union + 1e-7))\n    miou = iou.mean().item()\n    return {\"IoU_bg\": iou[0].item(), \"IoU_fg\": iou[1].item(), \"mIoU\": miou}\n\n@torch.no_grad()\ndef evaluate(dl):\n    model.eval()\n    cm = torch.zeros(2,2, device=DEVICE)\n    for x, _, q, _ in dl:\n        x = x.to(DEVICE); q = q.to(DEVICE)\n        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            out = model(x)[\"out\"].argmax(1)\n        for b in range(out.size(0)):\n            cm += fast_confusion(out[b], q[b])\n    return iou_from_cm(cm)\n\ndef train_one_epoch():\n    model.train()\n    tot = 0.0\n    for x, g, _, _ in train_dl:\n        x = x.to(DEVICE); g = g.to(DEVICE)\n        opt.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            logits = model(x)[\"out\"]\n            loss = ce_loss(logits, g)\n        scaler.scale(loss).backward()\n        scaler.step(opt); scaler.update()\n        tot += loss.item() * x.size(0)\n    return tot / len(train_dl.dataset)\n\n# train quick (match baseline: 15 epochs + early stop 3)\nEPOCHS, PATIENCE = 15, 3\nbest, wait = 0.0, 0\nfor ep in range(1, EPOCHS+1):\n    tl = train_one_epoch()\n    mets = evaluate(val_dl)\n    print(f\"Epoch {ep:02d}: loss={tl:.4f}  mIoU={mets['mIoU']:.3f} (bg={mets['IoU_bg']:.3f}, fg={mets['IoU_fg']:.3f})\")\n    if mets['mIoU'] > best + 1e-4:\n        best, wait = mets['mIoU'], 0\n        torch.save(model.state_dict(), CKPT_DIR / \"best.pth\")\n    else:\n        wait += 1\n        if wait > PATIENCE:\n            print(\"Early stop.\")\n            break\n\nprint(\"Best mIoU:\", round(best,3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T02:47:37.053470Z","iopub.execute_input":"2025-08-26T02:47:37.053830Z","iopub.status.idle":"2025-08-26T02:51:09.549027Z","shell.execute_reply.started":"2025-08-26T02:47:37.053798Z","shell.execute_reply":"2025-08-26T02:51:09.548222Z"}},"outputs":[{"name":"stdout","text":"Epoch 01: loss=0.4952  mIoU=0.557 (bg=0.716, fg=0.398)\nEpoch 02: loss=0.4583  mIoU=0.541 (bg=0.757, fg=0.324)\nEpoch 03: loss=0.4502  mIoU=0.506 (bg=0.738, fg=0.273)\nEpoch 04: loss=0.4418  mIoU=0.514 (bg=0.730, fg=0.297)\nEpoch 05: loss=0.4294  mIoU=0.528 (bg=0.752, fg=0.305)\nEarly stop.\nBest mIoU: 0.557\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# simple perturbations (eval only)\ndef apply_blur(img):     return cv2.GaussianBlur(img, (5,5), 1.0)\ndef apply_bright(img):   return np.clip(img*1.25, 0, 255).astype(np.uint8)\ndef apply_noise(img):    return np.clip(img + np.random.normal(0, 10, img.shape), 0, 255).astype(np.uint8)\ndef apply_hflip(img):    return cv2.flip(img, 1)\ndef apply_rotate(img):   # 10 degrees\n    h,w = img.shape[:2]\n    M = cv2.getRotationMatrix2D((w/2,h/2), 10, 1.0)\n    return cv2.warpAffine(img, M, (w,h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n\nPERTURBS = {\n    \"clean\":    lambda im: im,\n    \"blur\":     apply_blur,\n    \"brightness\": apply_bright,\n    \"gauss\":    apply_noise,\n    \"hflip\":    apply_hflip,\n    \"rotation\": apply_rotate,\n}\n\n@torch.no_grad()\ndef eval_perturbations(val_ids):\n    rows = []\n    model.eval()\n    for name, fn in PERTURBS.items():\n        cm = torch.zeros(2,2, device=DEVICE)\n        for img_id in val_ids:\n            ip = IMG_ROOT / f\"{img_id}.jpg\"\n            if not ip.exists(): ip = IMG_ROOT / f\"{img_id}.jpeg\"\n            img = np.array(Image.open(ip).convert(\"RGB\"))\n            gt_p = GT_ROOT / f\"{img_id}.png\"\n            gt = np.array(Image.open(gt_p)) if gt_p.exists() else np.full(img.shape[:2], IGNORE_IDX, np.uint8)\n\n            pim = fn(img)\n            pim = cv2.resize(pim, (IMG_SIZE, IMG_SIZE))\n            x = torch.from_numpy((pim/255.0 - np.array([0.485,0.456,0.406])) / np.array([0.229,0.224,0.225])).permute(2,0,1).float().unsqueeze(0).to(DEVICE)\n            q = cv2.resize(gt, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n            q = torch.from_numpy(np.where(q==IGNORE_IDX, IGNORE_IDX, (q!=0).astype(np.uint8))).long().to(DEVICE)\n\n            out = model(x)[\"out\"].argmax(1)[0]\n            cm += fast_confusion(out, q)\n        mets = iou_from_cm(cm)\n        rows.append({\"perturb\": name, **mets})\n        print(f\"{name:10s}  mIoU={mets['mIoU']:.3f}  (bg={mets['IoU_bg']:.3f}, fg={mets['IoU_fg']:.3f})\")\n\n    df = pd.DataFrame(rows)\n    df.to_csv(RESULTS_DIR / \"voc_val_robustness.csv\", index=False)\n    print(\"Saved:\", RESULTS_DIR / \"voc_val_robustness.csv\")\n    return df\n\ndf_rob = eval_perturbations(val_ids)\ndf_rob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T02:51:09.550376Z","iopub.execute_input":"2025-08-26T02:51:09.550604Z","iopub.status.idle":"2025-08-26T02:56:42.284708Z","shell.execute_reply.started":"2025-08-26T02:51:09.550582Z","shell.execute_reply":"2025-08-26T02:56:42.283914Z"}},"outputs":[{"name":"stdout","text":"clean       mIoU=0.528  (bg=0.752, fg=0.305)\nblur        mIoU=0.532  (bg=0.749, fg=0.315)\nbrightness  mIoU=0.515  (bg=0.752, fg=0.277)\ngauss       mIoU=0.508  (bg=0.753, fg=0.263)\nhflip       mIoU=0.474  (bg=0.713, fg=0.234)\nrotation    mIoU=0.527  (bg=0.741, fg=0.312)\nSaved: /kaggle/working/outputs/A_rotate/results/voc_val_robustness.csv\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"      perturb    IoU_bg    IoU_fg      mIoU\n0       clean  0.751711  0.305164  0.528438\n1        blur  0.749218  0.314530  0.531874\n2  brightness  0.751709  0.277320  0.514515\n3       gauss  0.752617  0.263381  0.507999\n4       hflip  0.713244  0.234416  0.473830\n5    rotation  0.741232  0.311783  0.526508","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>perturb</th>\n      <th>IoU_bg</th>\n      <th>IoU_fg</th>\n      <th>mIoU</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>clean</td>\n      <td>0.751711</td>\n      <td>0.305164</td>\n      <td>0.528438</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>blur</td>\n      <td>0.749218</td>\n      <td>0.314530</td>\n      <td>0.531874</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>brightness</td>\n      <td>0.751709</td>\n      <td>0.277320</td>\n      <td>0.514515</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gauss</td>\n      <td>0.752617</td>\n      <td>0.263381</td>\n      <td>0.507999</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>hflip</td>\n      <td>0.713244</td>\n      <td>0.234416</td>\n      <td>0.473830</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>rotation</td>\n      <td>0.741232</td>\n      <td>0.311783</td>\n      <td>0.526508</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"SAMPLE_DIR = BASE_OUT / \"samples\"; SAMPLE_DIR.mkdir(parents=True, exist_ok=True)\n\n@torch.no_grad()\ndef save_samples(n=6):\n    model.eval()\n    picks = val_ids[:n]\n    for img_id in picks:\n        ip = IMG_ROOT / f\"{img_id}.jpg\"\n        if not ip.exists(): ip = IMG_ROOT / f\"{img_id}.jpeg\"\n        img = Image.open(ip).convert(\"RGB\"); arr = np.array(img)\n        x = torchvision.transforms.functional.normalize(\n                torchvision.transforms.functional.to_tensor(img),\n                mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]\n            ).unsqueeze(0).to(DEVICE)\n        pred = model(x)[\"out\"].argmax(1)[0].cpu().numpy()  # 0/1\n        overlay = arr.copy()\n        overlay[pred==1] = (0.0*overlay[pred==1] + np.array([0,255,0])*0.8).astype(np.uint8)\n        Image.fromarray(overlay).save(SAMPLE_DIR / f\"{img_id}_overlay.png\")\n\nsave_samples(n=8)\nprint(\"Samples at:\", SAMPLE_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T03:10:06.393333Z","iopub.execute_input":"2025-08-26T03:10:06.394108Z","iopub.status.idle":"2025-08-26T03:10:20.060156Z","shell.execute_reply.started":"2025-08-26T03:10:06.394080Z","shell.execute_reply":"2025-08-26T03:10:20.059543Z"}},"outputs":[{"name":"stdout","text":"Samples at: /kaggle/working/outputs/A_rotate/samples\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!zip -r pseudo_masks_gradcam.zip outputs/A_rotate/pseudo_masks/","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r A_Rotate_samples_gradcam.zip outputs/A_rotate/samples/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T03:34:23.743330Z","iopub.execute_input":"2025-08-26T03:34:23.743620Z","iopub.status.idle":"2025-08-26T03:34:23.985720Z","shell.execute_reply.started":"2025-08-26T03:34:23.743593Z","shell.execute_reply":"2025-08-26T03:34:23.985023Z"}},"outputs":[{"name":"stdout","text":"updating: outputs/A_rotate/samples/ (stored 0%)\n  adding: outputs/A_rotate/samples/2007_000033_overlay.png (deflated 0%)\n  adding: outputs/A_rotate/samples/2007_000123_overlay.png (deflated 1%)\n  adding: outputs/A_rotate/samples/2007_000061_overlay.png (deflated 1%)\n  adding: outputs/A_rotate/samples/2007_000175_overlay.png (deflated 0%)\n  adding: outputs/A_rotate/samples/2007_000042_overlay.png (deflated 0%)\n  adding: outputs/A_rotate/samples/2007_000323_overlay.png (deflated 0%)\n  adding: outputs/A_rotate/samples/2007_000129_overlay.png (deflated 0%)\n  adding: outputs/A_rotate/samples/2007_000187_overlay.png (deflated 0%)\n","output_type":"stream"}],"execution_count":20}]}